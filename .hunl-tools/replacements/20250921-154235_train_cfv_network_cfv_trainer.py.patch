--- a/cfv_trainer.py
+++ b/cfv_trainer.py
@@ -4,83 +4,85 @@
 from torch import nn
 from torch.optim import Adam
 
-def train_cfv_network(model, train_samples, val_samples, epochs=350, batch_size=1000, lr=1e-3, lr_drop_epoch=200, lr_after=1e-4, device=None):
-	if device is None:
-		device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-	model = model.to(device)
-	optimizer = Adam(model.parameters(), lr=lr)
-	criterion = nn.SmoothL1Loss(reduction="mean")
-	K = int(getattr(model, "num_clusters", 0))
-	def _batcher(samples, bs, shuffle):
-		n = len(samples)
-		idx = list(range(n))
-		if shuffle:
-			random.shuffle(idx)
-		i = 0
-		while i < n:
-			j = min(i + bs, n)
-			b = [samples[k] for k in idx[i:j]]
-			i = j
-			x = []
-			y1 = []
-			y2 = []
-			for s in b:
-				x.append(s["input_vector"])
-				y1.append(s["target_v1"])
-				y2.append(s["target_v2"])
-			xt = torch.tensor(x, dtype=torch.float32, device=device)
-			y1t = torch.tensor(y1, dtype=torch.float32, device=device)
-			y2t = torch.tensor(y2, dtype=torch.float32, device=device)
-			yield xt, y1t, y2t
-	def _ranges_from_inputs(x):
-		start_r1 = 1 + 52
-		end_r1 = start_r1 + K
-		start_r2 = end_r1
-		end_r2 = start_r2 + K
-		r1 = x[:, start_r1:end_r1]
-		r2 = x[:, start_r2:end_r2]
-		return r1, r2
-	def _eval_loss(split):
-		model.eval()
-		total = 0.0
-		count = 0
-		with torch.no_grad():
-			for xb, y1b, y2b in _batcher(val_samples if split == "val" else train_samples, batch_size, shuffle=False):
-				r1b, r2b = _ranges_from_inputs(xb)
-				p1, p2 = model(xb)
-				f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
-				l1 = criterion(f1, y1b)
-				l2 = criterion(f2, y2b)
-				l = 0.5 * (l1 + l2)
-				total += float(l.item()) * xb.shape[0]
-				count += xb.shape[0]
-		return total / max(1, count)
-	best_state = None
-	best_val = math.inf
-	history = {"train": [], "val": []}
-	e = 0
-	while e < int(epochs):
-		if e == int(lr_drop_epoch):
-			for g in optimizer.param_groups:
-				g["lr"] = float(lr_after)
-		model.train()
-		for xb, y1b, y2b in _batcher(train_samples, batch_size, shuffle=True):
-			r1b, r2b = _ranges_from_inputs(xb)
-			p1, p2 = model(xb)
-			f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
-			l1 = criterion(f1, y1b)
-			l2 = criterion(f2, y2b)
-			loss = 0.5 * (l1 + l2)
-			optimizer.zero_grad(set_to_none=True)
-			loss.backward()
-			optimizer.step()
-		tr = _eval_loss("train")
-		va = _eval_loss("val")
-		history["train"].append(tr)
-		history["val"].append(va)
-		if va < best_val:
-			best_val = va
-			best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
-		e += 1
-	return {"best_state": best_state, "best_val_loss": float(best_val), "history": history}
+def train_cfv_network(model, train_samples, val_samples, epochs=350, batch_size=1000, lr=1e-3, lr_drop_epoch=200, lr_after=1e-4, device=None, seed=None):
+        if seed is not None:
+                set_global_seed(int(seed))
+        if device is None:
+                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+        model = model.to(device)
+        optimizer = Adam(model.parameters(), lr=lr)
+        criterion = nn.SmoothL1Loss(reduction="mean")
+        K = int(getattr(model, "num_clusters", 0))
+        def _batcher(samples, bs, shuffle):
+                n = len(samples)
+                idx = list(range(n))
+                if shuffle:
+                        random.shuffle(idx)
+                i = 0
+                while i < n:
+                        j = min(i + bs, n)
+                        b = [samples[k] for k in idx[i:j]]
+                        i = j
+                        x = []
+                        y1 = []
+                        y2 = []
+                        for s in b:
+                                x.append(s["input_vector"])
+                                y1.append(s["target_v1"])
+                                y2.append(s["target_v2"])
+                        xt = torch.tensor(x, dtype=torch.float32, device=device)
+                        y1t = torch.tensor(y1, dtype=torch.float32, device=device)
+                        y2t = torch.tensor(y2, dtype=torch.float32, device=device)
+                        yield xt, y1t, y2t
+        def _ranges_from_inputs(x):
+                start_r1 = 1 + 52
+                end_r1 = start_r1 + K
+                start_r2 = end_r1
+                end_r2 = start_r2 + K
+                r1 = x[:, start_r1:end_r1]
+                r2 = x[:, start_r2:end_r2]
+                return r1, r2
+        def _eval_loss(split):
+                model.eval()
+                total = 0.0
+                count = 0
+                with torch.no_grad():
+                        for xb, y1b, y2b in _batcher(val_samples if split == "val" else train_samples, batch_size, shuffle=False):
+                                r1b, r2b = _ranges_from_inputs(xb)
+                                p1, p2 = model(xb)
+                                f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
+                                l1 = criterion(f1, y1b)
+                                l2 = criterion(f2, y2b)
+                                l = 0.5 * (l1 + l2)
+                                total += float(l.item()) * xb.shape[0]
+                                count += xb.shape[0]
+                return total / max(1, count)
+        best_state = None
+        best_val = math.inf
+        history = {"train": [], "val": []}
+        e = 0
+        while e < int(epochs):
+                if e == int(lr_drop_epoch):
+                        for g in optimizer.param_groups:
+                                g["lr"] = float(lr_after)
+                model.train()
+                for xb, y1b, y2b in _batcher(train_samples, batch_size, shuffle=True):
+                        r1b, r2b = _ranges_from_inputs(xb)
+                        p1, p2 = model(xb)
+                        f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
+                        l1 = criterion(f1, y1b)
+                        l2 = criterion(f2, y2b)
+                        loss = 0.5 * (l1 + l2)
+                        optimizer.zero_grad(set_to_none=True)
+                        loss.backward()
+                        optimizer.step()
+                tr = _eval_loss("train")
+                va = _eval_loss("val")
+                history["train"].append(tr)
+                history["val"].append(va)
+                if va < best_val:
+                        best_val = va
+                        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+                e += 1
+        return {"best_state": best_state, "best_val_loss": float(best_val), "history": history}
 
