--- a/cfv_trainer.py
+++ b/cfv_trainer.py
@@ -253,99 +253,116 @@
         path_epoch,
     )
 
-	def train_flop_cfv(
-		model,
-		train_samples,
-		val_samples,
-		epochs=200,
-		batch_size=1000,
-		lr=1e-3,
-		lr_after=1e-4,
-		lr_drop_epoch=150,
-		weight_decay=1e-6,
-		device=None,
-		seed=None,
-		ckpt_dir=None,
-		save_best=True,
-		target_provider=None,
-		turn_model=None,
-		turn_device=None,
-	):
-		if seed is not None:
-			set_global_seed(int(seed))
-		if device is None:
-			device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-		model = model.to(device)
-		if turn_model is not None:
-			td = turn_device if turn_device is not None else device
-			turn_model = turn_model.to(td)
-			turn_model.eval()
-		optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
-		criterion = nn.SmoothL1Loss(reduction="mean")
-		K = int(getattr(model, "num_clusters", 0))
-		best_metric = None
-		best_state = None
-		history = {"train_huber": [], "val_huber": [], "val_mae": []}
-		e = 0
-		total_epochs = int(epochs)
-		while e < total_epochs:
-			_maybe_step_lr(optimizer, e, lr_drop_epoch, lr_after)
-			_train_one_epoch(
-				model=model,
-				train_samples=train_samples,
-				batch_size=batch_size,
-				device=device,
-				K=K,
-				criterion=criterion,
-				optimizer=optimizer,
-				target_provider=target_provider,
-				turn_model=turn_model,
-			)
-			tr_huber, _ = _epoch_eval(
-				model=model,
-				samples=train_samples,
-				batch_size=batch_size,
-				device=device,
-				K=K,
-				criterion=criterion,
-				target_provider=target_provider,
-				turn_model=turn_model,
-			)
-			val_set = val_samples if val_samples is not None else train_samples
-			val_huber, val_mae = _epoch_eval(
-				model=model,
-				samples=val_set,
-				batch_size=batch_size,
-				device=device,
-				K=K,
-				criterion=criterion,
-				target_provider=target_provider,
-				turn_model=turn_model,
-			)
-			history["train_huber"].append(tr_huber)
-			history["val_huber"].append(val_huber)
-			history["val_mae"].append(val_mae)
-			cur_metric = val_huber
-			best_metric, maybe_best_state = _maybe_save_best(
-				ckpt_dir=ckpt_dir,
-				e=e,
-				K=K,
-				save_best=save_best,
-				cur_metric=cur_metric,
-				best_metric=best_metric,
-				model=model,
-				val_huber=val_huber,
-				val_mae=val_mae,
-			)
-			if maybe_best_state is not None:
-				best_state = maybe_best_state
-			_save_epoch_ckpt(ckpt_dir=ckpt_dir, e=e, K=K, model=model, val_huber=val_huber, val_mae=val_mae)
-			e += 1
-		if save_best and best_state is not None:
-			model.load_state_dict(best_state)
-		final_state = best_state if best_state is not None else {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
-		return {"best_state": final_state, "history": history}
-
+    def train_flop_cfv(
+        model,
+        train_samples,
+        val_samples,
+        epochs=200,
+        batch_size=1000,
+        lr=1e-3,
+        lr_after=1e-4,
+        lr_drop_epoch=150,
+        weight_decay=1e-6,
+        device=None,
+        seed=None,
+        ckpt_dir=None,
+        save_best=True,
+        target_provider=None,
+        turn_model=None,
+        turn_device=None,
+    ):
+        if seed is not None:
+            set_global_seed(int(seed))
+        if device is None:
+            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+        model = model.to(device)
+        if target_provider is None and turn_model is not None:
+            if turn_device is None:
+                turn_device = device
+            turn_model = turn_model.to(turn_device)
+            turn_model.eval()
+            def _tp(xb, y1b, y2b, tm):
+                return default_turn_leaf_target_provider(xb.to(turn_device), y1b, y2b, tm)
+            target_provider = _tp
+        optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
+        criterion = nn.SmoothL1Loss(reduction="mean")
+        K = int(getattr(model, "num_clusters", 0))
+        history = {"train_huber": [], "val_huber": [], "val_mae": []}
+        best_metric = None
+        best_state = None
+        for e in range(int(epochs)):
+            if e == int(lr_drop_epoch):
+                for g in optimizer.param_groups:
+                    g["lr"] = float(lr_after)
+            model.train()
+            for xb, y1b, y2b in _cfv_batcher(train_samples, int(batch_size), shuffle=True, device=device):
+                if target_provider is not None:
+                    t1, t2 = target_provider(xb, y1b, y2b, turn_model) if turn_model is not None else target_provider(xb, y1b, y2b, None)
+                    y1t = t1.to(device)
+                    y2t = t2.to(device)
+                else:
+                    y1t = y1b
+                    y2t = y2b
+                r1b, r2b = _ranges_from_inputs(xb, K)
+                p1, p2 = model(xb)
+                f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
+                l1 = criterion(f1, y1t)
+                l2 = criterion(f2, y2t)
+                loss = 0.5 * (l1 + l2)
+                optimizer.zero_grad(set_to_none=True)
+                loss.backward()
+                optimizer.step()
+            model.eval()
+            with torch.no_grad():
+                if target_provider is not None:
+                    def _prep_targets(samples):
+                        buf = []
+                        for xb, y1b, y2b in _cfv_batcher(samples, int(batch_size), shuffle=False, device=device):
+                            t1, t2 = target_provider(xb, y1b, y2b, turn_model) if turn_model is not None else target_provider(xb, y1b, y2b, None)
+                            buf.append((xb, t1.to(device), t2.to(device)))
+                        return buf
+                    train_buf = _prep_targets(train_samples)
+                    val_buf = _prep_targets(val_samples if val_samples is not None else train_samples)
+                    def _eval_buf(buf):
+                        total_huber = 0.0
+                        total_mae = 0.0
+                        count = 0
+                        for xb, t1, t2 in buf:
+                            r1b, r2b = _ranges_from_inputs(xb, K)
+                            p1, p2 = model(xb)
+                            f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
+                            l1 = criterion(f1, t1)
+                            l2 = criterion(f2, t2)
+                            l = 0.5 * (l1 + l2)
+                            mae = 0.5 * (torch.mean(torch.abs(f1 - t1)) + torch.mean(torch.abs(f2 - t2)))
+                            bs = xb.shape[0]
+                            total_huber += float(l.item()) * bs
+                            total_mae += float(mae.item()) * bs
+                            count += bs
+                        den = max(1, count)
+                        return total_huber / den, total_mae / den
+                    tr_huber, _ = _eval_buf(train_buf)
+                    val_huber, val_mae = _eval_buf(val_buf)
+                else:
+                    tr_huber, _ = _eval_loss_cfv(model, train_samples, val_samples, split="train", batch_size=int(batch_size), device=device, criterion=criterion, K=K), 0.0
+                    val_huber, val_mae = _eval_loss_cfv(model, train_samples, val_samples if val_samples is not None else train_samples, split="val", batch_size=int(batch_size), device=device, criterion=criterion, K=K), 0.0
+            history["train_huber"].append(float(tr_huber))
+            history["val_huber"].append(float(val_huber))
+            history["val_mae"].append(float(val_mae))
+            cur = float(val_huber)
+            if save_best and (best_metric is None or cur < best_metric):
+                best_metric = cur
+                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+                if ckpt_dir:
+                    path = f"{ckpt_dir.rstrip('/')}/flop_cfv_best.pt"
+                    torch.save({"epoch": int(e), "val_huber": float(val_huber), "val_mae": float(val_mae), "state_dict": best_state, "num_clusters": int(K)}, path)
+            if ckpt_dir:
+                path_epoch = f"{ckpt_dir.rstrip('/')}/flop_cfv_epoch_{int(e)}.pt"
+                torch.save({"epoch": int(e), "val_huber": float(val_huber), "val_mae": float(val_mae), "state_dict": {k: v.detach().cpu() for k, v in model.state_dict().items()}, "num_clusters": int(K)}, path_epoch)
+        if save_best and best_state is not None:
+            model.load_state_dict(best_state)
+        final_state = best_state if best_state is not None else {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+        return {"best_state": final_state, "history": history}
 
 def train_turn_cfv_streaming(model, train_iter, val_iter=None, epochs=200, batch_size=1000, lr=1e-3, lr_after=1e-4, lr_drop_epoch=150, weight_decay=1e-6, device=None, seed=None, ckpt_dir=None, save_best=True):
 	if seed is not None:
