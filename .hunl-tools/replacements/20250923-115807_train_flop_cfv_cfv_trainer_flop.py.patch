--- a/cfv_trainer_flop.py
+++ b/cfv_trainer_flop.py
@@ -11,152 +11,160 @@
 from poker_utils import DECK
 
 def train_flop_cfv(
-		model,
-		train_samples,
-		val_samples,
-		epochs=200,
-		batch_size=1000,
-		lr=1e-3,
-		lr_after=1e-4,
-		lr_drop_epoch=150,
-		weight_decay=1e-6,
-		device=None,
-		seed=None,
-		ckpt_dir=None,
-		save_best=True,
-		target_provider=None,
-		turn_model=None,
-		turn_device=None,
-		):
-		if seed is not None:
-				random.seed(int(seed))
-				torch.manual_seed(int(seed))
-				if torch.cuda.is_available():
-						torch.cuda.manual_seed_all(int(seed))
-		if device is None:
-				device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-		model = model.to(device)
-		if target_provider is None and turn_model is not None:
-				if turn_device is None:
-						turn_device = device
-				turn_model = turn_model.to(turn_device)
-				turn_model.eval()
-				def _tp(xb, y1b, y2b, tm):
-						return default_turn_leaf_target_provider(xb.to(turn_device), y1b, y2b, tm)
-				target_provider = _tp
-		optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
-		criterion = nn.SmoothL1Loss(reduction="mean")
-		K = int(getattr(model, "num_clusters", 0))
-		history = {"train_huber": [], "val_huber": [], "val_mae": [], "val_residual_max": []}
-		best_metric = None
-		best_state = None
-		for e in range(int(epochs)):
-				_maybe_step_lr(optimizer, e, lr_drop_epoch, lr_after)
-				_train_one_epoch(
-						model=model,
-						train_samples=train_samples,
-						batch_size=int(batch_size),
-						device=device,
-						K=K,
-						criterion=criterion,
-						optimizer=optimizer,
-						target_provider=target_provider,
-						turn_model=turn_model,
-				)
-				model.eval()
-				with torch.no_grad():
-						if target_provider is not None:
-								def _prep_targets(samples):
-										total_huber = 0.0
-										total_mae = 0.0
-										count = 0
-										residual_max = 0.0
-										n = len(samples)
-										i = 0
-										while i < n:
-												j = min(i + int(batch_size), n)
-												chunk = samples[i:j]
-												i = j
-												xb, y1b, y2b = _tensorize_batch(chunk, device)
-												t1, t2 = (
-														target_provider(xb, y1b, y2b, turn_model)
-														if turn_model is not None
-														else target_provider(xb, y1b, y2b, None)
-												)
-												r1b, r2b = _ranges_from_inputs_inline(xb, K)
-												p1, p2 = model(xb)
-												f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
-												l1 = criterion(f1, t1)
-												l2 = criterion(f2, t2)
-												l = 0.5 * (l1 + l2)
-												mae = 0.5 * (torch.mean(torch.abs(f1 - t1)) + torch.mean(torch.abs(f2 - t2)))
-												s1 = torch.sum(r1b * f1, dim=1)
-												s2 = torch.sum(r2b * f2, dim=1)
-												res = torch.abs(s1 + s2)
-												bs = xb.shape[0]
-												total_huber += float(l.item()) * bs
-												total_mae += float(mae.item()) * bs
-												count += bs
-												mx = float(torch.max(res).item()) if res.numel() > 0 else 0.0
-												if mx > residual_max:
-														residual_max = mx
-										den = max(1, count)
-										return total_huber / den, total_mae / den, residual_max
-								tr_huber, _, _ = _prep_targets(train_samples)
-								val_base = val_samples if val_samples is not None else train_samples
-								val_huber, val_mae, val_resmax = _prep_targets(val_base)
-						else:
-								tr_huber, _, _ = _epoch_eval(
-										model=model,
-										samples=train_samples,
-										batch_size=int(batch_size),
-										device=device,
-										K=K,
-										criterion=criterion,
-										target_provider=None,
-										turn_model=None,
-								)
-								val_base = val_samples if val_samples is not None else train_samples
-								val_huber, val_mae, val_resmax = _epoch_eval(
-										model=model,
-										samples=val_base,
-										batch_size=int(batch_size),
-										device=device,
-										K=K,
-										criterion=criterion,
-										target_provider=None,
-										turn_model=None,
-								)
-				history["train_huber"].append(float(tr_huber))
-				history["val_huber"].append(float(val_huber))
-				history["val_mae"].append(float(val_mae))
-				history["val_residual_max"].append(float(val_resmax))
-				cur = float(val_huber)
-				if save_best and (best_metric is None or cur < best_metric):
-						best_metric = cur
-						best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
-						if ckpt_dir:
-								_maybe_save_best(
-										ckpt_dir=ckpt_dir,
-										e=e,
-										K=K,
-										save_best=True,
-										cur_metric=cur,
-										best_metric=None,
-										model=model,
-										val_huber=val_huber,
-										val_mae=val_mae,
-								)
-				if ckpt_dir:
-						_save_epoch_ckpt(ckpt_dir, e, K, model, val_huber, val_mae)
-		if save_best and best_state is not None:
-				model.load_state_dict(best_state)
-		final_state = (
-				best_state
-				if best_state is not None
-				else {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
-		)
-		return {"best_state": final_state, "history": history}
+        model,
+        train_samples,
+        val_samples,
+        epochs=200,
+        batch_size=1000,
+        lr=1e-3,
+        lr_after=1e-4,
+        lr_drop_epoch=150,
+        weight_decay=1e-6,
+        device=None,
+        seed=None,
+        ckpt_dir=None,
+        save_best=True,
+        target_provider=None,
+        turn_model=None,
+        turn_device=None,
+        early_stop_patience=30,
+        min_delta=0.0,
+        ):
+        if seed is not None:
+                random.seed(int(seed))
+                torch.manual_seed(int(seed))
+                if torch.cuda.is_available():
+                        torch.cuda.manual_seed_all(int(seed))
+        if device is None:
+                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+        model = model.to(device)
+        if target_provider is None and turn_model is not None:
+                if turn_device is None:
+                        turn_device = device
+                turn_model = turn_model.to(turn_device)
+                turn_model.eval()
+                def _tp(xb, y1b, y2b, tm):
+                        return default_turn_leaf_target_provider(xb.to(turn_device), y1b, y2b, tm)
+                target_provider = _tp
+        optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
+        criterion = nn.SmoothL1Loss(reduction="mean")
+        K = int(getattr(model, "num_clusters", 0))
+        history = {"train_huber": [], "val_huber": [], "val_mae": [], "val_residual_max": []}
+        best_metric = None
+        best_state = None
+        pat = 0
+        for e in range(int(epochs)):
+                _maybe_step_lr(optimizer, e, lr_drop_epoch, lr_after)
+                _train_one_epoch(
+                        model=model,
+                        train_samples=train_samples,
+                        batch_size=int(batch_size),
+                        device=device,
+                        K=K,
+                        criterion=criterion,
+                        optimizer=optimizer,
+                        target_provider=target_provider,
+                        turn_model=turn_model,
+                )
+                model.eval()
+                with torch.no_grad():
+                        if target_provider is not None:
+                                def _prep_targets(samples):
+                                        total_huber = 0.0
+                                        total_mae = 0.0
+                                        count = 0
+                                        residual_max = 0.0
+                                        n = len(samples)
+                                        i = 0
+                                        while i < n:
+                                                j = min(i + int(batch_size), n)
+                                                chunk = samples[i:j]
+                                                i = j
+                                                xb, y1b, y2b = _tensorize_batch(chunk, device)
+                                                t1, t2 = (
+                                                        target_provider(xb, y1b, y2b, turn_model)
+                                                        if turn_model is not None
+                                                        else target_provider(xb, y1b, y2b, None)
+                                                )
+                                                r1b, r2b = _ranges_from_inputs_inline(xb, K)
+                                                p1, p2 = model(xb)
+                                                f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
+                                                l1 = criterion(f1, t1)
+                                                l2 = criterion(f2, t2)
+                                                l = 0.5 * (l1 + l2)
+                                                mae = 0.5 * (torch.mean(torch.abs(f1 - t1)) + torch.mean(torch.abs(f2 - t2)))
+                                                s1 = torch.sum(r1b * f1, dim=1)
+                                                s2 = torch.sum(r2b * f2, dim=1)
+                                                res = torch.abs(s1 + s2)
+                                                bs = xb.shape[0]
+                                                total_huber += float(l.item()) * bs
+                                                total_mae += float(mae.item()) * bs
+                                                count += bs
+                                                mx = float(torch.max(res).item()) if res.numel() > 0 else 0.0
+                                                if mx > residual_max:
+                                                        residual_max = mx
+                                        den = max(1, count)
+                                        return total_huber / den, total_mae / den, residual_max
+                                tr_huber, _, _ = _prep_targets(train_samples)
+                                val_base = val_samples if val_samples is not None else train_samples
+                                val_huber, val_mae, val_resmax = _prep_targets(val_base)
+                        else:
+                                tr_huber, _, _ = _epoch_eval(
+                                        model=model,
+                                        samples=train_samples,
+                                        batch_size=int(batch_size),
+                                        device=device,
+                                        K=K,
+                                        criterion=criterion,
+                                        target_provider=None,
+                                        turn_model=None,
+                                )
+                                val_base = val_samples if val_samples is not None else train_samples
+                                val_huber, val_mae, val_resmax = _epoch_eval(
+                                        model=model,
+                                        samples=val_base,
+                                        batch_size=int(batch_size),
+                                        device=device,
+                                        K=K,
+                                        criterion=criterion,
+                                        target_provider=None,
+                                        turn_model=None,
+                                )
+                history["train_huber"].append(float(tr_huber))
+                history["val_huber"].append(float(val_huber))
+                history["val_mae"].append(float(val_mae))
+                history["val_residual_max"].append(float(val_resmax))
+                cur = float(val_huber)
+                if save_best and (best_metric is None or cur < best_metric):
+                        best_metric = cur
+                        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+                        if ckpt_dir:
+                                _maybe_save_best(
+                                        ckpt_dir=ckpt_dir,
+                                        e=e,
+                                        K=K,
+                                        save_best=True,
+                                        cur_metric=cur,
+                                        best_metric=None,
+                                        model=model,
+                                        val_huber=val_huber,
+                                        val_mae=val_mae,
+                                )
+                        pat = 0
+                else:
+                        pat += 1
+                        if int(pat) >= int(early_stop_patience):
+                                break
+                if ckpt_dir:
+                        _save_epoch_ckpt(ckpt_dir, e, K, model, val_huber, val_mae)
+        if save_best and best_state is not None:
+                model.load_state_dict(best_state)
+        final_state = (
+                best_state
+                if best_state is not None
+                else {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+        )
+        return {"best_state": final_state, "history": history}
 
 def default_turn_leaf_target_provider(xb, y1b, y2b, turn_model):
 		device_turn = next(turn_model.parameters()).device
