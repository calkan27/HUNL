--- a/cfv_trainer.py
+++ b/cfv_trainer.py
@@ -20,92 +20,82 @@
 
 
 def train_cfv_network(
-	model,
-	train_samples,
-	val_samples,
-	epochs=350,
-	batch_size=1000,
-	lr=1e-3,
-	lr_drop_epoch=200,
-	lr_after=1e-4,
-	weight_decay=1e-6,
-	device=None,
-	seed=None,
-	early_stop_patience=30,
-	min_delta=0.0,
+        model,
+        train_samples,
+        val_samples,
+        epochs=350,
+        batch_size=1000,
+        lr=1e-3,
+        lr_drop_epoch=200,
+        lr_after=1e-4,
+        weight_decay=1e-6,
+        device=None,
+        seed=None,
+        early_stop_patience=30,
+        min_delta=0.0,
 ):
-
-
-	if seed is not None:
-		set_global_seed(int(seed))
-
-	if device is None:
-		device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-	model = model.to(device)
-
-	optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
-	criterion = nn.SmoothL1Loss(reduction="mean")
-	K = int(getattr(model, "num_clusters", 0))
-
-	best_state = None
-	best_val = math.inf
-	history = {"train_huber": [], "train_mae": [], "val_huber": [], "val_mae": []}
-	pat = 0
-
-	for e in range(int(epochs)):
-		if e == int(lr_drop_epoch):
-			for g in optimizer.param_groups:
-				g["lr"] = float(lr_after)
-
-		model.train()
-		for xb, y1b, y2b in _cfv_batcher(train_samples, int(batch_size), shuffle=True, device=device):
-			r1b, r2b = _ranges_from_inputs(xb, K)
-			p1, p2 = model(xb)
-			f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
-			l1 = criterion(f1, y1b)
-			l2 = criterion(f2, y2b)
-			loss = 0.5 * (l1 + l2)
-			optimizer.zero_grad(set_to_none=True)
-			loss.backward()
-			optimizer.step()
-
-		tr_huber, tr_mae = _eval_loss_cfv(
-			model=model,
-			train_samples=train_samples,
-			val_samples=val_samples,
-			split="train",
-			batch_size=int(batch_size),
-			device=device,
-			criterion=criterion,
-			K=K,
-		)
-		val_huber, val_mae = _eval_loss_cfv(
-			model=model,
-			train_samples=train_samples,
-			val_samples=val_samples,
-			split="val",
-			batch_size=int(batch_size),
-			device=device,
-			criterion=criterion,
-			K=K,
-		)
-
-		history["train_huber"].append(float(tr_huber))
-		history["train_mae"].append(float(tr_mae))
-		history["val_huber"].append(float(val_huber))
-		history["val_mae"].append(float(val_mae))
-
-		if val_huber + float(min_delta) < best_val:
-			best_val = float(val_huber)
-			best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
-			pat = 0
-		else:
-			pat += 1
-			if pat >= int(early_stop_patience):
-				break
-
-	return {"best_state": best_state, "best_val_loss": float(best_val), "history": history}
-
+        if seed is not None:
+                set_global_seed(int(seed))
+        if device is None:
+                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+        model = model.to(device)
+        optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
+        criterion = nn.SmoothL1Loss(reduction="mean")
+        K = int(getattr(model, "num_clusters", 0))
+        best_state = None
+        best_val = math.inf
+        history = {"train_huber": [], "train_mae": [], "train_residual_max": [], "val_huber": [], "val_mae": [], "val_residual_max": []}
+        pat = 0
+        for e in range(int(epochs)):
+                if e == int(lr_drop_epoch):
+                        for g in optimizer.param_groups:
+                                g["lr"] = float(lr_after)
+                model.train()
+                for xb, y1b, y2b in _cfv_batcher(train_samples, int(batch_size), shuffle=True, device=device):
+                        r1b, r2b = _ranges_from_inputs(xb, K)
+                        p1, p2 = model(xb)
+                        f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
+                        l1 = criterion(f1, y1b)
+                        l2 = criterion(f2, y2b)
+                        loss = 0.5 * (l1 + l2)
+                        optimizer.zero_grad(set_to_none=True)
+                        loss.backward()
+                        optimizer.step()
+                tr_huber, tr_mae, tr_resmax = _eval_loss_cfv(
+                        model=model,
+                        train_samples=train_samples,
+                        val_samples=val_samples,
+                        split="train",
+                        batch_size=int(batch_size),
+                        device=device,
+                        criterion=criterion,
+                        K=K,
+                )
+                val_huber, val_mae, val_resmax = _eval_loss_cfv(
+                        model=model,
+                        train_samples=train_samples,
+                        val_samples=val_samples,
+                        split="val",
+                        batch_size=int(batch_size),
+                        device=device,
+                        criterion=criterion,
+                        K=K,
+                )
+                history["train_huber"].append(float(tr_huber))
+                history["train_mae"].append(float(tr_mae))
+                history["train_residual_max"].append(float(tr_resmax))
+                history["val_huber"].append(float(val_huber))
+                history["val_mae"].append(float(val_mae))
+                history["val_residual_max"].append(float(val_resmax))
+                if val_huber + float(min_delta) < best_val:
+                        best_val = float(val_huber)
+                        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+                        pat = 0
+                else:
+                        pat += 1
+                        if pat >= int(early_stop_patience):
+                                break
+        return {"best_state": best_state, "best_val_loss": float(best_val), "history": history}
 
 def _cfv_batcher(samples, batch_size, shuffle, device):
 
