--- a/cfr_solver.py
+++ b/cfr_solver.py
@@ -506,60 +506,61 @@
             cache[node_key] = counterfactual_values
             return counterfactual_values
 
-	def _calculate_counterfactual_utility(self, node, player, depth):
-			if self._is_terminal(node):
-					return self._calculate_terminal_utility(node, player)
-			stage = self.get_stage(node)
-			if depth >= self.depth_limit and stage in ('preflop', 'flop', 'turn'):
-					preds = self.predict_counterfactual_values(node, player)
-					ev = 0.0
-					total = 0.0
-					for cid, p in node.player_ranges[player].items():
-							if cid in preds:
-									v = preds[cid][0]
-									if isinstance(v, list) or isinstance(v, tuple):
-											val = float(v[0])
-									else:
-											val = float(v)
-									ev += p * val
-									total += p
-					if total > 0:
-							return ev / total
-					else:
-							return 0.0
-			current_player = node.current_player
-			opponent = (player + 1) % 2
-			expected_value = 0.0
-			if current_player == player:
-					allowed_actions = self._allowed_actions_agent(node.public_state)
-					for cluster_id, cluster_prob in node.player_ranges[player].items():
-							if cluster_prob == 0.0:
-									continue
-							values = self.cfr_values[node]
-							base_strategy = values.compute_strategy(cluster_id)
-							strategy = self._mask_strategy(base_strategy, allowed_actions)
-							action_utilities = []
-							for a_type in allowed_actions:
-									a_idx = a_type.value
-									if a_idx in values.pruned_actions[cluster_id]:
-											action_utilities.append(0.0)
-											continue
-									action = Action(a_type)
-									new_public_state = node.public_state.update_state(node, action)
-									child_node = GameNode(new_public_state)
-									child_node.player_ranges = copy.deepcopy(node.player_ranges)
-									self.update_player_range(child_node, player, cluster_id, a_idx)
-									utility = self._calculate_counterfactual_utility(child_node, player, depth + 1)
-									action_utilities.append(utility)
-							i = 0
-							while i < len(allowed_actions):
-									a_idx = allowed_actions[i].value
-									expected_value += cluster_prob * strategy[a_idx] * action_utilities[i]
-									i += 1
-			else:
-					worst_case_for_us = self._opponent_node_value_from_upper_bounds(node, agent_player=player)
-					expected_value += worst_case_for_us
-			return expected_value
+    def _calculate_counterfactual_utility(self, node, player, depth):
+            if self._is_terminal(node):
+                    return self._calculate_terminal_utility(node, player)
+            stage = self.get_stage(node)
+            if depth >= self.depth_limit and stage in ('preflop', 'flop', 'turn'):
+                    preds = self.predict_counterfactual_values(node, player)
+                    scale = float(node.public_state.pot_size)
+                    ev = 0.0
+                    total = 0.0
+                    for cid, p in node.player_ranges[player].items():
+                            if cid in preds:
+                                    v = preds[cid][0]
+                                    if isinstance(v, list) or isinstance(v, tuple):
+                                            val = float(v[0]) * scale
+                                    else:
+                                            val = float(v) * scale
+                                    ev += p * val
+                                    total += p
+                    if total > 0:
+                            return ev / total
+                    else:
+                            return 0.0
+            current_player = node.current_player
+            opponent = (player + 1) % 2
+            expected_value = 0.0
+            if current_player == player:
+                    allowed_actions = self._allowed_actions_agent(node.public_state)
+                    for cluster_id, cluster_prob in node.player_ranges[player].items():
+                            if cluster_prob == 0.0:
+                                    continue
+                            values = self.cfr_values[node]
+                            base_strategy = values.compute_strategy(cluster_id)
+                            strategy = self._mask_strategy(base_strategy, allowed_actions)
+                            action_utilities = []
+                            for a_type in allowed_actions:
+                                    a_idx = a_type.value
+                                    if a_idx in values.pruned_actions[cluster_id]:
+                                            action_utilities.append(0.0)
+                                            continue
+                                    action = Action(a_type)
+                                    new_public_state = node.public_state.update_state(node, action)
+                                    child_node = GameNode(new_public_state)
+                                    child_node.player_ranges = copy.deepcopy(node.player_ranges)
+                                    self.update_player_range(child_node, player, cluster_id, a_idx)
+                                    utility = self._calculate_counterfactual_utility(child_node, player, depth + 1)
+                                    action_utilities.append(utility)
+                            i = 0
+                            while i < len(allowed_actions):
+                                    a_idx = allowed_actions[i].value
+                                    expected_value += cluster_prob * strategy[a_idx] * action_utilities[i]
+                                    i += 1
+            else:
+                    worst_case_for_us = self._opponent_node_value_from_upper_bounds(node, agent_player=player)
+                    expected_value += worst_case_for_us
+            return expected_value
 
 	def update_player_range(self, node, player, cluster_id, action_index):
 			values = self.cfr_values[node]
