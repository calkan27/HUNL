--- a/cfv_trainer_flop.py
+++ b/cfv_trainer_flop.py
@@ -11,163 +11,152 @@
 from poker_utils import DECK
 
 def train_flop_cfv(
-	model,
-	train_samples,
-	val_samples,
-	epochs=200,
-	batch_size=1000,
-	lr=1e-3,
-	lr_after=1e-4,
-	lr_drop_epoch=150,
-	weight_decay=1e-6,
-	device=None,
-	seed=None,
-	ckpt_dir=None,
-	save_best=True,
-	target_provider=None,
-	turn_model=None,
-	turn_device=None,
-	):
-
-	if seed is not None:
-		random.seed(int(seed))
-		torch.manual_seed(int(seed))
-		if torch.cuda.is_available():
-			torch.cuda.manual_seed_all(int(seed))
-
-	if device is None:
-		device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-	model = model.to(device)
-
-	if target_provider is None and turn_model is not None:
-		if turn_device is None:
-			turn_device = device
-		turn_model = turn_model.to(turn_device)
-		turn_model.eval()
-
-		def _tp(xb, y1b, y2b, tm):
-			return default_turn_leaf_target_provider(xb.to(turn_device), y1b, y2b, tm)
-
-		target_provider = _tp
-
-	optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
-	criterion = nn.SmoothL1Loss(reduction="mean")
-	K = int(getattr(model, "num_clusters", 0))
-
-	history = {"train_huber": [], "val_huber": [], "val_mae": []}
-	best_metric = None
-	best_state = None
-
-	for e in range(int(epochs)):
-		_maybe_step_lr(optimizer, e, lr_drop_epoch, lr_after)
-
-		_train_one_epoch(
-			model=model,
-			train_samples=train_samples,
-			batch_size=int(batch_size),
-			device=device,
-			K=K,
-			criterion=criterion,
-			optimizer=optimizer,
-			target_provider=target_provider,
-			turn_model=turn_model,
-		)
-
-		model.eval()
-		with torch.no_grad():
-			if target_provider is not None:
-				def _prep_targets(samples):
-					total_huber = 0.0
-					total_mae = 0.0
-					count = 0
-					n = len(samples)
-					i = 0
-					while i < n:
-						j = min(i + int(batch_size), n)
-						chunk = samples[i:j]
-						i = j
-						xb, y1b, y2b = _tensorize_batch(chunk, device)
-						t1, t2 = (
-							target_provider(xb, y1b, y2b, turn_model)
-							if turn_model is not None
-							else target_provider(xb, y1b, y2b, None)
-						)
-						r1b, r2b = _ranges_from_inputs_inline(xb, K)
-						p1, p2 = model(xb)
-						f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
-						l1 = criterion(f1, t1)
-						l2 = criterion(f2, t2)
-						l = 0.5 * (l1 + l2)
-						mae = 0.5 * (
-							torch.mean(torch.abs(f1 - t1)) + torch.mean(torch.abs(f2 - t2))
-						)
-						bs = xb.shape[0]
-						total_huber += float(l.item()) * bs
-						total_mae += float(mae.item()) * bs
-						count += bs
-					den = max(1, count)
-					return total_huber / den, total_mae / den
-
-				tr_huber, _ = _prep_targets(train_samples)
-				val_base = val_samples if val_samples is not None else train_samples
-				val_huber, val_mae = _prep_targets(val_base)
-			else:
-				tr_huber, _ = _epoch_eval(
-					model=model,
-					samples=train_samples,
-					batch_size=int(batch_size),
-					device=device,
-					K=K,
-					criterion=criterion,
-					target_provider=None,
-					turn_model=None,
-				)
-				val_base = val_samples if val_samples is not None else train_samples
-				val_huber, val_mae = _epoch_eval(
-					model=model,
-					samples=val_base,
-					batch_size=int(batch_size),
-					device=device,
-					K=K,
-					criterion=criterion,
-					target_provider=None,
-					turn_model=None,
-				)
-
-		history["train_huber"].append(float(tr_huber))
-		history["val_huber"].append(float(val_huber))
-		history["val_mae"].append(float(val_mae))
-
-		cur = float(val_huber)
-		if save_best and (best_metric is None or cur < best_metric):
-			best_metric = cur
-			best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
-			if ckpt_dir:
-				_maybe_save_best(
-					ckpt_dir=ckpt_dir,
-					e=e,
-					K=K,
-					save_best=True,
-					cur_metric=cur,
-					best_metric=None,  
-					model=model,
-					val_huber=val_huber,
-					val_mae=val_mae,
-				)
-
-		if ckpt_dir:
-			_save_epoch_ckpt(ckpt_dir, e, K, model, val_huber, val_mae)
-
-	if save_best and best_state is not None:
-		model.load_state_dict(best_state)
-
-	final_state = (
-		best_state
-		if best_state is not None
-		else {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
-	)
-	return {"best_state": final_state, "history": history}
-
+        model,
+        train_samples,
+        val_samples,
+        epochs=200,
+        batch_size=1000,
+        lr=1e-3,
+        lr_after=1e-4,
+        lr_drop_epoch=150,
+        weight_decay=1e-6,
+        device=None,
+        seed=None,
+        ckpt_dir=None,
+        save_best=True,
+        target_provider=None,
+        turn_model=None,
+        turn_device=None,
+        ):
+        if seed is not None:
+                random.seed(int(seed))
+                torch.manual_seed(int(seed))
+                if torch.cuda.is_available():
+                        torch.cuda.manual_seed_all(int(seed))
+        if device is None:
+                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+        model = model.to(device)
+        if target_provider is None and turn_model is not None:
+                if turn_device is None:
+                        turn_device = device
+                turn_model = turn_model.to(turn_device)
+                turn_model.eval()
+                def _tp(xb, y1b, y2b, tm):
+                        return default_turn_leaf_target_provider(xb.to(turn_device), y1b, y2b, tm)
+                target_provider = _tp
+        optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
+        criterion = nn.SmoothL1Loss(reduction="mean")
+        K = int(getattr(model, "num_clusters", 0))
+        history = {"train_huber": [], "val_huber": [], "val_mae": [], "val_residual_max": []}
+        best_metric = None
+        best_state = None
+        for e in range(int(epochs)):
+                _maybe_step_lr(optimizer, e, lr_drop_epoch, lr_after)
+                _train_one_epoch(
+                        model=model,
+                        train_samples=train_samples,
+                        batch_size=int(batch_size),
+                        device=device,
+                        K=K,
+                        criterion=criterion,
+                        optimizer=optimizer,
+                        target_provider=target_provider,
+                        turn_model=turn_model,
+                )
+                model.eval()
+                with torch.no_grad():
+                        if target_provider is not None:
+                                def _prep_targets(samples):
+                                        total_huber = 0.0
+                                        total_mae = 0.0
+                                        count = 0
+                                        residual_max = 0.0
+                                        n = len(samples)
+                                        i = 0
+                                        while i < n:
+                                                j = min(i + int(batch_size), n)
+                                                chunk = samples[i:j]
+                                                i = j
+                                                xb, y1b, y2b = _tensorize_batch(chunk, device)
+                                                t1, t2 = (
+                                                        target_provider(xb, y1b, y2b, turn_model)
+                                                        if turn_model is not None
+                                                        else target_provider(xb, y1b, y2b, None)
+                                                )
+                                                r1b, r2b = _ranges_from_inputs_inline(xb, K)
+                                                p1, p2 = model(xb)
+                                                f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
+                                                l1 = criterion(f1, t1)
+                                                l2 = criterion(f2, t2)
+                                                l = 0.5 * (l1 + l2)
+                                                mae = 0.5 * (torch.mean(torch.abs(f1 - t1)) + torch.mean(torch.abs(f2 - t2)))
+                                                s1 = torch.sum(r1b * f1, dim=1)
+                                                s2 = torch.sum(r2b * f2, dim=1)
+                                                res = torch.abs(s1 + s2)
+                                                bs = xb.shape[0]
+                                                total_huber += float(l.item()) * bs
+                                                total_mae += float(mae.item()) * bs
+                                                count += bs
+                                                mx = float(torch.max(res).item()) if res.numel() > 0 else 0.0
+                                                if mx > residual_max:
+                                                        residual_max = mx
+                                        den = max(1, count)
+                                        return total_huber / den, total_mae / den, residual_max
+                                tr_huber, _, _ = _prep_targets(train_samples)
+                                val_base = val_samples if val_samples is not None else train_samples
+                                val_huber, val_mae, val_resmax = _prep_targets(val_base)
+                        else:
+                                tr_huber, _, _ = _epoch_eval(
+                                        model=model,
+                                        samples=train_samples,
+                                        batch_size=int(batch_size),
+                                        device=device,
+                                        K=K,
+                                        criterion=criterion,
+                                        target_provider=None,
+                                        turn_model=None,
+                                )
+                                val_base = val_samples if val_samples is not None else train_samples
+                                val_huber, val_mae, val_resmax = _epoch_eval(
+                                        model=model,
+                                        samples=val_base,
+                                        batch_size=int(batch_size),
+                                        device=device,
+                                        K=K,
+                                        criterion=criterion,
+                                        target_provider=None,
+                                        turn_model=None,
+                                )
+                history["train_huber"].append(float(tr_huber))
+                history["val_huber"].append(float(val_huber))
+                history["val_mae"].append(float(val_mae))
+                history["val_residual_max"].append(float(val_resmax))
+                cur = float(val_huber)
+                if save_best and (best_metric is None or cur < best_metric):
+                        best_metric = cur
+                        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+                        if ckpt_dir:
+                                _maybe_save_best(
+                                        ckpt_dir=ckpt_dir,
+                                        e=e,
+                                        K=K,
+                                        save_best=True,
+                                        cur_metric=cur,
+                                        best_metric=None,
+                                        model=model,
+                                        val_huber=val_huber,
+                                        val_mae=val_mae,
+                                )
+                if ckpt_dir:
+                        _save_epoch_ckpt(ckpt_dir, e, K, model, val_huber, val_mae)
+        if save_best and best_state is not None:
+                model.load_state_dict(best_state)
+        final_state = (
+                best_state
+                if best_state is not None
+                else {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+        )
+        return {"best_state": final_state, "history": history}
 
 def default_turn_leaf_target_provider(xb, y1b, y2b, turn_model):
 		device_turn = next(turn_model.parameters()).device
