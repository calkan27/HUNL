--- a/cfr_solver.py
+++ b/cfr_solver.py
@@ -431,78 +431,60 @@
         cache[node_key] = counterfactual_values
         return counterfactual_values
 
-	def _calculate_counterfactual_utility(self, node, player, depth):
-		if self._is_terminal(node):
-			return self._calculate_terminal_utility(node, player)
-		stage = self.get_stage(node)
-		if depth >= self.depth_limit and stage in ('preflop', 'flop', 'turn'):
-			preds = self.predict_counterfactual_values(node, player)
-			ev = 0.0
-			total = 0.0
-			for cid, p in node.player_ranges[player].items():
-				if cid in preds:
-					v = preds[cid][0]
-					if isinstance(v, list) or isinstance(v, tuple):
-						val = float(v[0])
-					else:
-						val = float(v)
-					ev += p * val
-					total += p
-			if total > 0:
-				return ev / total
-			else:
-				return 0.0
-		current_player = node.current_player
-		opponent = (player + 1) % 2
-		expected_value = 0.0
-		if current_player == player:
-			allowed_actions = self._allowed_actions_agent(node.public_state)
-			for cluster_id, cluster_prob in node.player_ranges[player].items():
-				if cluster_prob == 0.0:
-					continue
-				values = self.cfr_values[node]
-				base_strategy = values.compute_strategy(cluster_id)
-				strategy = self._mask_strategy(base_strategy, allowed_actions)
-				action_utilities = []
-				for a_type in allowed_actions:
-					a_idx = a_type.value
-					if a_idx in values.pruned_actions[cluster_id]:
-						action_utilities.append(0.0)
-						continue
-					action = Action(a_type)
-					new_public_state = node.public_state.update_state(node, action)
-					child_node = GameNode(new_public_state)
-					child_node.player_ranges = copy.deepcopy(node.player_ranges)
-					self.update_player_range(child_node, player, cluster_id, a_idx)
-					utility = self._calculate_counterfactual_utility(child_node, player, depth + 1)
-					action_utilities.append(utility)
-				i = 0
-				while i < len(allowed_actions):
-					a_idx = allowed_actions[i].value
-					expected_value += cluster_prob * strategy[a_idx] * action_utilities[i]
-					i += 1
-		else:
-			allowed_actions = self._allowed_actions_opponent(node.public_state)
-			for cluster_id, cluster_prob in node.player_ranges[opponent].items():
-				if cluster_prob == 0.0:
-					continue
-				values = self.cfr_values[node]
-				base_strategy = values.compute_strategy(cluster_id)
-				strategy = self._mask_strategy(base_strategy, allowed_actions)
-				expected_action_value = 0.0
-				for a_type in allowed_actions:
-					a_idx = a_type.value
-					if a_idx in values.pruned_actions[cluster_id]:
-						continue
-					action = Action(a_type)
-					new_public_state = node.public_state.update_state(node, action)
-					child_node = GameNode(new_public_state)
-					child_node.player_ranges = copy.deepcopy(node.player_ranges)
-					self.update_player_range(child_node, opponent, cluster_id, a_idx)
-					utility = self._calculate_counterfactual_utility(child_node, player, depth + 1)
-					expected_action_value += strategy[a_idx] * utility
-				expected_value += cluster_prob * expected_action_value
-		return expected_value
+    def _calculate_counterfactual_utility(self, node, player, depth):
+        if self._is_terminal(node):
+            return self._calculate_terminal_utility(node, player)
+        stage = self.get_stage(node)
+        if depth >= self.depth_limit and stage in ('preflop', 'flop', 'turn'):
+            preds = self.predict_counterfactual_values(node, player)
+            ev = 0.0
+            total = 0.0
+            for cid, p in node.player_ranges[player].items():
+                if cid in preds:
+                    v = preds[cid][0]
+                    if isinstance(v, list) or isinstance(v, tuple):
+                        val = float(v[0])
+                    else:
+                        val = float(v)
+                    ev += p * val
+                    total += p
+            if total > 0:
+                return ev / total
+            else:
+                return 0.0
+        current_player = node.current_player
+        opponent = (player + 1) % 2
+        expected_value = 0.0
+        if current_player == player:
+            allowed_actions = self._allowed_actions_agent(node.public_state)
+            for cluster_id, cluster_prob in node.player_ranges[player].items():
+                if cluster_prob == 0.0:
+                    continue
+                values = self.cfr_values[node]
+                base_strategy = values.compute_strategy(cluster_id)
+                strategy = self._mask_strategy(base_strategy, allowed_actions)
+                action_utilities = []
+                for a_type in allowed_actions:
+                    a_idx = a_type.value
+                    if a_idx in values.pruned_actions[cluster_id]:
+                        action_utilities.append(0.0)
+                        continue
+                    action = Action(a_type)
+                    new_public_state = node.public_state.update_state(node, action)
+                    child_node = GameNode(new_public_state)
+                    child_node.player_ranges = copy.deepcopy(node.player_ranges)
+                    self.update_player_range(child_node, player, cluster_id, a_idx)
+                    utility = self._calculate_counterfactual_utility(child_node, player, depth + 1)
+                    action_utilities.append(utility)
+                i = 0
+                while i < len(allowed_actions):
+                    a_idx = allowed_actions[i].value
+                    expected_value += cluster_prob * strategy[a_idx] * action_utilities[i]
+                    i += 1
+        else:
+            worst_case_for_us = self._opponent_node_value_from_upper_bounds(node, agent_player=player)
+            expected_value += worst_case_for_us
+        return expected_value
 
 	def update_player_range(self, node, player, cluster_id, action_index):
 		values = self.cfr_values[node]
