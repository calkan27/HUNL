--- a/cfv_trainer.py
+++ b/cfv_trainer.py
@@ -15,83 +15,82 @@
 
 
 def train_cfv_network(
-		model,
-		train_samples,
-		val_samples,
-		epochs=350,
-		batch_size=1000,
-		lr=1e-3,
-		lr_drop_epoch=200,
-		lr_after=1e-4,
-		weight_decay=1e-6,
-		device=None,
-		seed=None,
-		early_stop_patience=30,
-		min_delta=0.0,
+        model,
+        train_samples,
+        val_samples,
+        epochs=350,
+        batch_size=1000,
+        lr=1e-3,
+        lr_drop_epoch=200,
+        lr_after=1e-4,
+        weight_decay=1e-6,
+        device=None,
+        seed=None,
+        early_stop_patience=30,
+        min_delta=0.0,
 ):
-	if seed is not None:
-			set_global_seed(int(seed))
-	if device is None:
-			device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-	model = model.to(device)
-	optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
-	criterion = nn.SmoothL1Loss(reduction="mean")
-	K = int(getattr(model, "num_clusters", 0))
-	best_state = None
-	best_val = math.inf
-	history = {"train_huber": [], "train_mae": [], "train_residual_max": [], "val_huber": [], "val_mae": [], "val_residual_max": []}
-	pat = 0
-	es_eps = 1e-6
-	for e in range(int(epochs)):
-			if e == int(lr_drop_epoch):
-					for g in optimizer.param_groups:
-							g["lr"] = float(lr_after)
-			model.train()
-			for xb, y1b, y2b in _cfv_batcher(train_samples, int(batch_size), shuffle=True, device=device):
-					r1b, r2b = _ranges_from_inputs(xb, K)
-					p1, p2 = model(xb)
-					f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
-					l1 = criterion(f1, y1b)
-					l2 = criterion(f2, y2b)
-					loss = 0.5 * (l1 + l2)
-					optimizer.zero_grad(set_to_none=True)
-					loss.backward()
-					optimizer.step()
-			tr_huber, tr_mae, tr_resmax = _eval_loss_cfv(
-					model=model,
-					train_samples=train_samples,
-					val_samples=val_samples,
-					split="train",
-					batch_size=int(batch_size),
-					device=device,
-					criterion=criterion,
-					K=K,
-			)
-			val_huber, val_mae, val_resmax = _eval_loss_cfv(
-					model=model,
-					train_samples=train_samples,
-					val_samples=val_samples,
-					split="val",
-					batch_size=int(batch_size),
-					device=device,
-					criterion=criterion,
-					K=K,
-			)
-			history["train_huber"].append(float(tr_huber))
-			history["train_mae"].append(float(tr_mae))
-			history["train_residual_max"].append(float(tr_resmax))
-			history["val_huber"].append(float(val_huber))
-			history["val_mae"].append(float(val_mae))
-			history["val_residual_max"].append(float(val_resmax))
-			if float(val_huber) < float(best_val) - max(float(min_delta), es_eps):
-					best_val = float(val_huber)
-					best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
-					pat = 0
-			else:
-					pat += 1
-					if pat >= int(early_stop_patience):
-							break
-	return {"best_state": best_state, "best_val_loss": float(best_val), "history": history}
+        if seed is not None:
+                set_global_seed(int(seed))
+        if device is None:
+                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+        model = model.to(device)
+        optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
+        criterion = nn.SmoothL1Loss(reduction="mean")
+        K = int(getattr(model, "num_clusters", 0))
+        best_state = None
+        best_val = math.inf
+        history = {"train_huber": [], "train_mae": [], "train_residual_max": [], "val_huber": [], "val_mae": [], "val_residual_max": []}
+        pat = 0
+        for e in range(int(epochs)):
+                if e == int(lr_drop_epoch):
+                        for g in optimizer.param_groups:
+                                g["lr"] = float(lr_after)
+                model.train()
+                for xb, y1b, y2b in _cfv_batcher(train_samples, int(batch_size), shuffle=True, device=device):
+                        r1b, r2b = _ranges_from_inputs(xb, K)
+                        p1, p2 = model(xb)
+                        f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
+                        l1 = criterion(f1, y1b)
+                        l2 = criterion(f2, y2b)
+                        loss = 0.5 * (l1 + l2)
+                        optimizer.zero_grad(set_to_none=True)
+                        loss.backward()
+                        optimizer.step()
+                tr_huber, tr_mae, tr_resmax = _eval_loss_cfv(
+                        model=model,
+                        train_samples=train_samples,
+                        val_samples=val_samples,
+                        split="train",
+                        batch_size=int(batch_size),
+                        device=device,
+                        criterion=criterion,
+                        K=K,
+                )
+                val_huber, val_mae, val_resmax = _eval_loss_cfv(
+                        model=model,
+                        train_samples=train_samples,
+                        val_samples=val_samples,
+                        split="val",
+                        batch_size=int(batch_size),
+                        device=device,
+                        criterion=criterion,
+                        K=K,
+                )
+                history["train_huber"].append(float(tr_huber))
+                history["train_mae"].append(float(tr_mae))
+                history["train_residual_max"].append(float(tr_resmax))
+                history["val_huber"].append(float(val_huber))
+                history["val_mae"].append(float(val_mae))
+                history["val_residual_max"].append(float(val_resmax))
+                if float(val_huber) + float(min_delta) < float(best_val):
+                        best_val = float(val_huber)
+                        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+                        pat = 0
+                else:
+                        pat += 1
+                        if pat >= int(early_stop_patience):
+                                break
+        return {"best_state": best_state, "best_val_loss": float(best_val), "history": history}
 
 def _cfv_batcher(samples, batch_size, shuffle, device):
 
