--- a/eval_cli.py
+++ b/eval_cli.py
@@ -20,61 +20,63 @@
 
 
 def _play_episode(
-	solver0: CFRSolver,
-	solver1: CFRSolver,
-	rng_seed: int,
-	value_solver_for_aivat: CFRSolver,
-	policy_iters_agent: int = 2,
-	policy_iters_opp: int = 1,
+        solver0: CFRSolver,
+        solver1: CFRSolver,
+        rng_seed: int,
+        value_solver_for_aivat: CFRSolver,
+        policy_iters_agent: int = 2,
+        policy_iters_opp: int = 1,
+        dealer: int = 0,
 ) -> Tuple[float, Dict]:
 
-	ps = _make_initial_preflop(stack=200, seed=rng_seed)
-	node = GameNode(ps)
+        ps = PublicState(initial_stacks=[200, 200], board_cards=[], dealer=int(dealer))
+        ps.current_round = 0
+        ps.current_player = ps.dealer
+        node = GameNode(ps)
 
-	K = solver0.num_clusters
-	u = 1.0 / float(K) if K > 0 else 0.0
-	node.player_ranges[0] = {i: u for i in range(K)}
-	node.player_ranges[1] = {i: u for i in range(K)}
+        K = solver0.num_clusters
+        u = 1.0 / float(K) if K > 0 else 0.0
+        node.player_ranges[0] = {i: u for i in range(K)}
+        node.player_ranges[1] = {i: u for i in range(K)}
 
-	pol_agent = _policy_from_resolve(solver0, iters=policy_iters_agent)
-	pol_opp = _policy_from_resolve(solver1, iters=policy_iters_opp)
+        pol_agent = _policy_from_resolve(solver0, iters=policy_iters_agent)
+        pol_opp = _policy_from_resolve(solver1, iters=policy_iters_opp)
 
-	events = []
-	step_guard = 0
-	while not node.public_state.is_terminal and step_guard < 200:
-		step_guard += 1
-		cur = node.public_state.current_player
-		if cur == 0:
-			dist = pol_agent(node, player=0)
-			chosen = _sample_from_policy(dist)
-			events.append({"type": "agent", "action": chosen, "policy": {k: float(v) for k, v in dist.items()}})
-			new_ps = node.public_state.update_state(node, Action(chosen))
-			node = GameNode(new_ps)
-			node.player_ranges = [dict(node.player_ranges[0]), dict(node.player_ranges[1])]
-		else:
-			dist = pol_opp(node, player=1)
-			chosen = _sample_from_policy(dist)
-			events.append({"type": "opponent", "action": chosen, "policy": {k: float(v) for k, v in dist.items()}})
-			new_ps = node.public_state.update_state(node, Action(chosen))
-			node = GameNode(new_ps)
-			node.player_ranges = [dict(node.player_ranges[0]), dict(node.player_ranges[1])]
+        events = []
+        step_guard = 0
+        while not node.public_state.is_terminal and step_guard < 200:
+                step_guard += 1
+                cur = node.public_state.current_player
+                if cur == 0:
+                        dist = pol_agent(node, player=0)
+                        chosen = _sample_from_policy(dist)
+                        events.append({"type": "agent", "action": chosen, "policy": {k: float(v) for k, v in dist.items()}})
+                        new_ps = node.public_state.update_state(node, Action(chosen))
+                        node = GameNode(new_ps)
+                        node.player_ranges = [dict(node.player_ranges[0]), dict(node.player_ranges[1])]
+                else:
+                        dist = pol_opp(node, player=1)
+                        chosen = _sample_from_policy(dist)
+                        events.append({"type": "opponent", "action": chosen, "policy": {k: float(v) for k, v in dist.items()}})
+                        new_ps = node.public_state.update_state(node, Action(chosen))
+                        node = GameNode(new_ps)
+                        node.player_ranges = [dict(node.player_ranges[0]), dict(node.player_ranges[1])]
 
-	if not node.public_state.is_terminal:
-		return 0.0, {"initial_node": None, "events": []}
+        if not node.public_state.is_terminal:
+                return 0.0, {"initial_node": None, "events": []}
 
-	naive = solver0._calculate_terminal_utility(node, player=0)
+        naive = solver0._calculate_terminal_utility(node, player=0)
 
-	episode = {
-		"initial_node": GameNode(_make_initial_preflop(stack=200, seed=rng_seed)),
-		"events": events,
-	}
+        episode = {
+                "initial_node": GameNode(PublicState(initial_stacks=[200, 200], board_cards=[], dealer=int(dealer))),
+                "events": events,
+        }
 
-	value_fn = _value_fn_from_solver(value_solver_for_aivat)
-	policy_fn = lambda nd, player: (pol_agent(nd, player) if player == 0 else pol_opp(nd, player))
-	aiv = AIVATEvaluator(value_fn=value_fn, policy_fn=policy_fn, chance_policy_fn=None, agent_player=0)
-	res = aiv.evaluate(episode)
-	return float(naive), {"aivat": float(res["aivat"])}
-
+        value_fn = _value_fn_from_solver(value_solver_for_aivat)
+        policy_fn = lambda nd, player: (pol_agent(nd, player) if player == 0 else pol_opp(nd, player))
+        aiv = AIVATEvaluator(value_fn=value_fn, policy_fn=policy_fn, chance_policy_fn=None, agent_player=0)
+        res = aiv.evaluate(episode)
+        return float(naive), {"aivat": float(res["aivat"])}
 
 def _run_matches(mode: str, episodes: int, seed: int, cfg: ResolveConfig) -> List[Tuple[float, float]]:
 
