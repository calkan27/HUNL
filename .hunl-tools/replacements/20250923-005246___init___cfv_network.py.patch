--- a/cfv_network.py
+++ b/cfv_network.py
@@ -9,32 +9,32 @@
 import torch.nn as nn
 
 class CounterfactualValueNetwork(nn.Module):
-	def __init__(self, input_size, num_clusters=1000, input_layout=None):
-		super(CounterfactualValueNetwork, self).__init__()
-		self.num_clusters = int(num_clusters)
-		self.input_size = int(input_size)
-		self.input_layout = dict(input_layout) if input_layout is not None else None
-		self.hidden_layer_1 = nn.Linear(self.input_size, 500)
-		self.activation_1 = nn.PReLU()
-		self.hidden_layer_2 = nn.Linear(500, 500)
-		self.activation_2 = nn.PReLU()
-		self.hidden_layer_3 = nn.Linear(500, 500)
-		self.activation_3 = nn.PReLU()
-		self.hidden_layer_4 = nn.Linear(500, 500)
-		self.activation_4 = nn.PReLU()
-		self.hidden_layer_5 = nn.Linear(500, 500)
-		self.activation_5 = nn.PReLU()
-		self.hidden_layer_6 = nn.Linear(500, 500)
-		self.activation_6 = nn.PReLU()
-		self.hidden_layer_7 = nn.Linear(500, 500)
-		self.activation_7 = nn.PReLU()
-		self.output_player1_values = nn.Linear(500, self.num_clusters)
-		self.output_player2_values = nn.Linear(500, self.num_clusters)
-		for m in self.modules():
-			if isinstance(m, nn.Linear):
-				nn.init.kaiming_normal_(m.weight, nonlinearity="linear")
-				if m.bias is not None:
-					nn.init.zeros_(m.bias)
+    def __init__(self, input_size, num_clusters=1000, input_layout=None):
+        super(CounterfactualValueNetwork, self).__init__()
+        self.num_clusters = int(num_clusters)
+        self.input_size = int(input_size)
+        self.input_layout = dict(input_layout) if input_layout is not None else None
+        self.hidden_layer_1 = nn.Linear(self.input_size, 500)
+        self.activation_1 = nn.PReLU()
+        self.hidden_layer_2 = nn.Linear(500, 500)
+        self.activation_2 = nn.PReLU()
+        self.hidden_layer_3 = nn.Linear(500, 500)
+        self.activation_3 = nn.PReLU()
+        self.hidden_layer_4 = nn.Linear(500, 500)
+        self.activation_4 = nn.PReLU()
+        self.hidden_layer_5 = nn.Linear(500, 500)
+        self.activation_5 = nn.PReLU()
+        self.hidden_layer_6 = nn.Linear(500, 500)
+        self.activation_6 = nn.PReLU()
+        self.hidden_layer_7 = nn.Linear(500, 500)
+        self.activation_7 = nn.PReLU()
+        self.output_player1_values = nn.Linear(500, self.num_clusters)
+        self.output_player2_values = nn.Linear(500, self.num_clusters)
+        for m in self.modules():
+            if isinstance(m, nn.Linear):
+                nn.init.kaiming_normal_(m.weight, nonlinearity="leaky_relu", a=0.25)
+                if m.bias is not None:
+                    nn.init.zeros_(m.bias)
 
 	def forward(self, input_tensor):
 		h = self.activation_1(self.hidden_layer_1(input_tensor))
