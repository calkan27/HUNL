--- a/cfv_trainer.py
+++ b/cfv_trainer.py
@@ -59,49 +59,48 @@
 			count += xb.shape[0]
 	return total / max(1, count)
 
-	def train_cfv_network(model, train_samples, val_samples, epochs=350, batch_size=1000, lr=1e-3, lr_drop_epoch=200, lr_after=1e-4, weight_decay=1e-6, device=None, seed=None, early_stop_patience=30, min_delta=0.0):
-		if seed is not None:
-			set_global_seed(int(seed))
-		if device is None:
-			device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-		model = model.to(device)
-		optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
-		criterion = nn.SmoothL1Loss(reduction="mean")
-		K = int(getattr(model, "num_clusters", 0))
-		best_state = None
-		best_val = math.inf
-		history = {"train": [], "val": []}
-		pat = 0
-		e = 0
-		while e < int(epochs):
-			if e == int(lr_drop_epoch):
-				for g in optimizer.param_groups:
-					g["lr"] = float(lr_after)
-			model.train()
-			for xb, y1b, y2b in _cfv_batcher(train_samples, batch_size, shuffle=True, device=device):
-				r1b, r2b = _ranges_from_inputs(xb, K)
-				p1, p2 = model(xb)
-				f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
-				l1 = criterion(f1, y1b)
-				l2 = criterion(f2, y2b)
-				loss = 0.5 * (l1 + l2)
-				optimizer.zero_grad(set_to_none=True)
-				loss.backward()
-				optimizer.step()
-			tr = _eval_loss_cfv(model, train_samples, val_samples, split="train", batch_size=batch_size, device=device, criterion=criterion, K=K)
-			va = _eval_loss_cfv(model, train_samples, val_samples, split="val", batch_size=batch_size, device=device, criterion=criterion, K=K)
-			history["train"].append(tr)
-			history["val"].append(va)
-			if va + float(min_delta) < best_val:
-				best_val = va
-				best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
-				pat = 0
-			else:
-				pat += 1
-				if pat >= int(early_stop_patience):
-					break
-			e += 1
-		return {"best_state": best_state, "best_val_loss": float(best_val), "history": history}
+    def train_cfv_network(model, train_samples, val_samples, epochs=350, batch_size=1000, lr=1e-3, lr_drop_epoch=200, lr_after=1e-4, weight_decay=1e-6, device=None, seed=None, early_stop_patience=30, min_delta=0.0):
+        if seed is not None:
+            set_global_seed(int(seed))
+        if device is None:
+            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+        model = model.to(device)
+        optimizer = Adam(model.parameters(), lr=lr, weight_decay=float(weight_decay))
+        criterion = nn.SmoothL1Loss(reduction="mean")
+        K = int(getattr(model, "num_clusters", 0))
+        best_state = None
+        best_val = math.inf
+        history = {"train": [], "val": []}
+        pat = 0
+        for e in range(int(epochs)):
+            if e == int(lr_drop_epoch):
+                for g in optimizer.param_groups:
+                    g["lr"] = float(lr_after)
+            model.train()
+            for xb, y1b, y2b in _cfv_batcher(train_samples, int(batch_size), shuffle=True, device=device):
+                r1b, r2b = _ranges_from_inputs(xb, K)
+                p1, p2 = model(xb)
+                f1, f2 = model.enforce_zero_sum(r1b, r2b, p1, p2)
+                l1 = criterion(f1, y1b)
+                l2 = criterion(f2, y2b)
+                loss = 0.5 * (l1 + l2)
+                optimizer.zero_grad(set_to_none=True)
+                loss.backward()
+                optimizer.step()
+            tr = _eval_loss_cfv(model, train_samples, val_samples, split="train", batch_size=int(batch_size), device=device, criterion=criterion, K=K)
+            va = _eval_loss_cfv(model, train_samples, val_samples, split="val", batch_size=int(batch_size), device=device, criterion=criterion, K=K)
+            history["train"].append(float(tr))
+            history["val"].append(float(va))
+            if va + float(min_delta) < best_val:
+                best_val = float(va)
+                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
+                pat = 0
+            else:
+                pat += 1
+                if pat >= int(early_stop_patience):
+                    break
+        return {"best_state": best_state, "best_val_loss": float(best_val), "history": history}
+
 	def train_turn_cfv(model, train_samples, val_samples, epochs=200, batch_size=1000, lr=1e-3, lr_after=1e-4, lr_drop_epoch=150, weight_decay=1e-6, device=None, seed=None, ckpt_dir=None, save_best=True):
 		if seed is not None:
 			set_global_seed(int(seed))
