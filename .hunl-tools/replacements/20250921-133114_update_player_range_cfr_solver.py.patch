--- a/cfr_solver.py
+++ b/cfr_solver.py
@@ -487,586 +487,42 @@
         return expected_value
 
     def update_player_range(self, node, player, cluster_id, action_index):
-        values = self.cfr_values[node]
-        num_actions = len(ActionType)
-        if player not in (0, 1):
-            return
-        priors = node.player_ranges[player]
-        post = {}
-        norm = 0.0
-        for cid, prior in priors.items():
-            current_strategy = values.compute_strategy(cid) if cid in values.cumulative_positive_regret else values.strategy.get(cid, [1.0 / num_actions] * num_actions)
-            if player == (node.public_state.current_player + 1) % 2:
-                like = 1.0
+            values = self.cfr_values[node]
+            num_actions = len(ActionType)
+            if player not in (0, 1):
+                    return
+            priors = node.player_ranges[player]
+            post = {}
+            norm = 0.0
+            for cid, prior in priors.items():
+                    current_strategy = values.compute_strategy(cid) if cid in values.cumulative_positive_regret else values.strategy.get(cid, [1.0 / num_actions] * num_actions)
+                    if player == (node.public_state.current_player + 1) % 2:
+                            like = 1.0
+                    else:
+                            if action_index < 0 or action_index >= len(current_strategy):
+                                    like = 0.0
+                            else:
+                                    like = current_strategy[action_index]
+                    weight = prior * like
+                    post[cid] = weight
+                    norm = norm + weight
+            if norm <= 0.0:
+                    total_prior = 0.0
+                    for cid in priors:
+                            total_prior = total_prior + priors[cid]
+                    if total_prior <= 0.0:
+                            k = len(priors)
+                            if k > 0:
+                                    u = 1.0 / k
+                                    for cid in post:
+                                            post[cid] = u
+                            else:
+                                    post = {}
+                    else:
+                            for cid in post:
+                                    post[cid] = priors[cid] / total_prior
             else:
-                if action_index < 0 or action_index >= len(current_strategy):
-                    like = 0.0
-                else:
-                    like = current_strategy[action_index]
-            weight = prior * like
-            post[cid] = weight
-            norm = norm + weight
-        if norm <= 0.0:
-            total_prior = 0.0
-            for cid in priors:
-                total_prior = total_prior + priors[cid]
-            if total_prior <= 0.0:
-                k = len(priors)
-                if k > 0:
-                    u = 1.0 / k
                     for cid in post:
-                        post[cid] = u
-                else:
-                    post = {}
-            else:
-                for cid in post:
-                    post[cid] = priors[cid] / total_prior
-        else:
-            for cid in post:
-                post[cid] = post[cid] / norm
-        node.player_ranges[player] = post
-
-	def _update_regret(self, node, player, counterfactual_values):
-		values = self.cfr_values[node]
-		num_actions = len(ActionType)
-		for cluster_id in node.player_ranges[player]:
-			strategy = values.compute_strategy(cluster_id)
-			cfv = counterfactual_values[cluster_id]
-			expected_utility = 0
-			for action_index in range(num_actions):
-				contribution = strategy[action_index] * cfv[action_index]
-				expected_utility += contribution
-			for action_index in range(num_actions):
-				if action_index in values.pruned_actions[cluster_id]:
-					continue
-				regret = cfv[action_index] - expected_utility
-				values.cumulative_regret[cluster_id][action_index] += regret
-				values.cumulative_positive_regret[cluster_id][action_index] = max(
-					values.cumulative_positive_regret[cluster_id][action_index] + regret, 0.0)
-				values.regret_squared_sums[cluster_id][action_index] += regret ** 2
-			values.update_strategy(cluster_id, strategy)
-
-	def _is_terminal(self, node):
-		return node.public_state.is_terminal
-
-	def _is_chance_node(self, node):
-		return False
-
-	def _handle_chance_node(self, node, player, depth):
-		return self._calculate_counterfactual_utility(node, player, depth + 1)
-
-	def _sample_board_cards(self, public_state):
-		return []
-
-	def _calculate_terminal_utility(self, node, player):
-		if not node.public_state.is_terminal:
-			return 0.0
-		if node.public_state.is_showdown:
-			return self._calculate_showdown_utility(node, player)
-		else:
-			return self._calculate_non_showdown_utility(node, player)
-
-	def _calculate_showdown_utility(self, node, player):
-		opponent = (player + 1) % 2
-		utility = 0.0
-
-		for player_cluster_id, player_cluster_prob in node.player_ranges[player].items():
-			player_hands = self.clusters.get(player_cluster_id, [])
-			n_p = len(player_hands)
-			if n_p == 0 or player_cluster_prob == 0.0:
-				continue
-			p_hand_weight = player_cluster_prob / float(n_p)
-
-			for opponent_cluster_id, opponent_cluster_prob in node.player_ranges[opponent].items():
-				opp_hands = self.clusters.get(opponent_cluster_id, [])
-				n_o = len(opp_hands)
-				if n_o == 0 or opponent_cluster_prob == 0.0:
-					continue
-				o_hand_weight = opponent_cluster_prob / float(n_o)
-
-				for player_hand in player_hands:
-					player_hand_cards = player_hand.split()
-					for opponent_hand in opp_hands:
-						opponent_hand_cards = opponent_hand.split()
-						result = self._player_wins(player_hand_cards, opponent_hand_cards, node.public_state.board_cards)
-						prob = p_hand_weight * o_hand_weight
-						if result == 1:
-							net_gain = node.public_state.pot_size - node.public_state.current_bets[player]
-							utility += prob * net_gain
-						elif result == -1:
-							net_loss = node.public_state.current_bets[player]
-							utility -= prob * net_loss
-						else:
-							net_gain = (node.public_state.pot_size / 2.0) - node.public_state.current_bets[player]
-							utility += prob * net_gain
-		return utility
-
-
-	def _player_wins(self, player_hand, opponent_hand, board_cards):
-		player_best_hand = best_hand(player_hand + board_cards)
-		opponent_best_hand = best_hand(opponent_hand + board_cards)
-		player_rank = hand_rank(player_best_hand)
-		opponent_rank = hand_rank(opponent_best_hand)
-		if player_rank > opponent_rank:
-			return 1
-		elif player_rank < opponent_rank:
-			return -1
-		else:
-			return 0
-
-	def _calculate_non_showdown_utility(self, node, player):
-		opponent = (player + 1) % 2
-		if not node.public_state.players_in_hand[opponent]:
-			return node.public_state.pot_size - node.public_state.current_bets[player]
-		elif not node.public_state.players_in_hand[player]:
-			return -node.public_state.current_bets[player]
-		else:
-			return 0.0
-
-	def predict_counterfactual_values(self, node, player):
-		stage = self.get_stage(node)
-		if stage == 'river':
-			def wins_fn(ph, oh, board):
-				return self._player_wins(ph, oh, board)
-			cf = self.river_endgame.compute_cluster_cfvs(self.clusters, node, player, wins_fn, best_hand, hand_rank)
-			out = {}
-			for cid, val in cf.items():
-				out[int(cid)] = val
-			return out
-		if stage not in self.models:
-			counterfactual_values = {}
-			for cluster_id in node.player_ranges[player]:
-				counterfactual_values[cluster_id] = [0.0] * len(ActionType)
-			return counterfactual_values
-		if stage == 'preflop':
-			input_vector = self.prepare_input_vector_preflop(node)
-		else:
-			input_vector = self.prepare_input_vector(node)
-		input_tensor = torch.tensor([input_vector], dtype=torch.float32).to(self.device)
-		stage_model = self.models[stage]
-		with torch.no_grad():
-			v1, v2 = stage_model(input_tensor)
-			K = self.num_clusters
-			if stage == 'preflop':
-				start_r1 = 1
-				end_r1 = start_r1 + K
-				start_r2 = end_r1
-				end_r2 = start_r2 + K
-			else:
-				start_r1 = 1 + len(DECK)
-				end_r1 = start_r1 + K
-				start_r2 = end_r1
-				end_r2 = start_r2 + K
-			r1 = input_tensor[:, start_r1:end_r1]
-			r2 = input_tensor[:, start_r2:end_r2]
-			v1_adj, v2_adj = stage_model.enforce_zero_sum(r1, r2, v1, v2)
-		pred = v1_adj if player == 0 else v2_adj
-		counterfactual_values = {}
-		for cluster_id in node.player_ranges[player]:
-			idx = int(cluster_id)
-			if 0 <= idx < self.num_clusters:
-				scalar = float(pred[0][idx].item())
-			else:
-				s = 0.0
-				scalar = s
-			counterfactual_values[cluster_id] = [scalar] * len(ActionType)
-		return counterfactual_values
-
-
-
-	def prepare_input_vector(self, node):
-		total_initial = sum(node.public_state.initial_stacks) if getattr(node.public_state, "initial_stacks", None) else 1.0
-		if total_initial <= 0:
-			total_initial = 1.0
-		pot_norm = node.public_state.pot_size / float(total_initial)
-		pot_vec = [pot_norm]
-		board_vec = self.encode_public_cards(node.public_state.board_cards)
-		K = self.num_clusters
-		r1 = [0.0] * K
-		r2 = [0.0] * K
-		range_p1 = node.player_ranges[0]
-		range_p2 = node.player_ranges[1]
-		total1 = sum(range_p1.values())
-		total2 = sum(range_p2.values())
-		for cluster_id, prob in range_p1.items():
-			if 0 <= int(cluster_id) < K and total1 > 0:
-				r1[int(cluster_id)] = prob / total1
-		for cluster_id, prob in range_p2.items():
-			if 0 <= int(cluster_id) < K and total2 > 0:
-				r2[int(cluster_id)] = prob / total2
-		return pot_vec + board_vec + r1 + r2
-
-
-
-
-
-	def encode_public_cards(self, public_cards):
-		card_encoding = {card: idx for idx, card in enumerate(DECK)}
-		public_card_vector = [0.0] * len(DECK)
-		for card in public_cards:
-			index = card_encoding[card]
-			public_card_vector[index] = 1.0
-		return public_card_vector
-
-	def encode_action_history(self, actions, max_history_length=10):
-		num_possible_actions = len(ActionType)
-		action_vector = [0.0] * (num_possible_actions * max_history_length)
-		for i, (player, action) in enumerate(actions[-max_history_length:]):
-			action_index = action.action_type.value
-			position = i * num_possible_actions + action_index
-			action_vector[position] = 1.0
-		return action_vector
-
-	def get_stage(self, node):
-		current_round = node.public_state.current_round
-		if current_round == 0:
-			return 'preflop'
-		elif current_round == 1:
-			return 'flop'
-		elif current_round == 2:
-			return 'turn'
-		elif current_round == 3:
-			return 'river'
-		else:
-			return 'none'
-
-
-
-	def _range_sig(self, r):
-		items = [(int(k), float(v)) for k, v in r.items()]
-		items.sort(key=lambda x: x[0])
-		return tuple((k, round(v, 12)) for k, v in items)
-
-	def _preflop_signature(self, node):
-		ps = node.public_state
-		tag = "preflop"
-		sb = int(ps.small_blind)
-		bb = int(ps.big_blind)
-		dealer = int(ps.dealer)
-		init_stacks = tuple(int(x) for x in (ps.initial_stacks if getattr(ps, "initial_stacks", None) else ps.stacks))
-		curr_stacks = tuple(int(x) for x in ps.stacks[0:2])
-		curr_bets = tuple(int(x) for x in ps.current_bets[0:2])
-		pot = int(ps.pot_size)
-		cluster_summary = tuple(sorted((int(cid), int(len(hset))) for cid, hset in (self.clusters.items() if self.clusters else [])))
-		r1_sig = self._range_sig(node.player_ranges[0]) if node.player_ranges and len(node.player_ranges) > 0 else tuple()
-		r2_sig = self._range_sig(node.player_ranges[1]) if node.player_ranges and len(node.player_ranges) > 1 else tuple()
-		K = int(self.num_clusters)
-		return (tag, sb, bb, dealer, init_stacks, curr_stacks, curr_bets, pot, K, cluster_summary, r1_sig, r2_sig)
-
-
-	def _build_allowed_actions(self, ps):
-		out = []
-		p = ps.current_player
-		o = (p + 1) % 2
-		my_bet = ps.current_bets[p]
-		opp_bet = ps.current_bets[o]
-		to_call = opp_bet - my_bet
-		if to_call < 0:
-			to_call = 0
-		mode = getattr(self._config, "bet_size_mode", "sparse_2") if getattr(self, "_config", None) is not None else "sparse_2"
-		if to_call > 0:
-			out.append(ActionType.FOLD)
-		out.append(ActionType.CALL)
-		min_raise_inc = ps._min_raise_size()
-		if to_call == 0:
-			if ps.stacks[p] > 0:
-				half = max(min_raise_inc, int(ps.pot_size * 0.5))
-				if ps.stacks[p] >= half and half > 0:
-					out.append(ActionType.HALF_POT_BET)
-				pot = max(min_raise_inc, int(ps.pot_size))
-				if ps.stacks[p] >= pot and pot > 0:
-					out.append(ActionType.POT_SIZED_BET)
-				if mode == "sparse_3":
-					two = max(min_raise_inc, int(ps.pot_size * 2.0))
-					if ps.stacks[p] >= two and two > 0:
-						out.append(ActionType.TWO_POT_BET)
-				out.append(ActionType.ALL_IN)
-		else:
-			remaining_after_call = ps.stacks[p] - to_call
-			if remaining_after_call > 0:
-				pot_after_call = ps.pot_size + to_call
-				half = max(min_raise_inc, int(pot_after_call * 0.5))
-				if remaining_after_call >= half and half > 0:
-					out.append(ActionType.HALF_POT_BET)
-				pot = max(min_raise_inc, int(pot_after_call))
-				if remaining_after_call >= pot and pot > 0:
-					out.append(ActionType.POT_SIZED_BET)
-				if mode == "sparse_3":
-					two = max(min_raise_inc, int(pot_after_call * 2.0))
-					if remaining_after_call >= two and two > 0:
-						out.append(ActionType.TWO_POT_BET)
-			out.append(ActionType.ALL_IN)
-		return out
-
-
-
-	def _mask_strategy(self, strategy, allowed_actions):
-		m = [0.0] * len(strategy)
-		allowed_idx = [a.value for a in allowed_actions]
-		s = 0.0
-		i = 0
-		while i < len(strategy):
-			if i in allowed_idx:
-				v = strategy[i]
-				if v < 0.0:
-					v = 0.0
-				m[i] = v
-				s += v
-			i += 1
-		if s > 0.0:
-			i = 0
-			while i < len(m):
-				m[i] = m[i] / s
-				i += 1
-		else:
-			u = 1.0 / float(len(allowed_idx)) if allowed_idx else 0.0
-			i = 0
-			while i < len(m):
-				m[i] = u if i in allowed_idx else 0.0
-				i += 1
-		return m
-
-	def _allowed_actions_agent(self, ps):
-		out = []
-		p = ps.current_player
-		o = (p + 1) % 2
-		my_bet = ps.current_bets[p]
-		opp_bet = ps.current_bets[o]
-		to_call = opp_bet - my_bet
-		if to_call < 0:
-			to_call = 0
-		mode = getattr(self._config, "bet_size_mode", "sparse_2") if getattr(self, "_config", None) is not None else "sparse_2"
-		min_raise_inc = ps._min_raise_size()
-		if to_call > 0:
-			out.append(ActionType.FOLD)
-		out.append(ActionType.CALL)
-		if to_call == 0:
-			if ps.stacks[p] > 0:
-				half = max(min_raise_inc, int(ps.pot_size * 0.5))
-				if ps.stacks[p] >= half and half > 0:
-					out.append(ActionType.HALF_POT_BET)
-				pot = max(min_raise_inc, int(ps.pot_size))
-				if ps.stacks[p] >= pot and pot > 0:
-					out.append(ActionType.POT_SIZED_BET)
-				if mode == "sparse_3":
-					two = max(min_raise_inc, int(ps.pot_size * 2.0))
-					if ps.stacks[p] >= two and two > 0:
-						out.append(ActionType.TWO_POT_BET)
-				out.append(ActionType.ALL_IN)
-		else:
-			remaining_after_call = ps.stacks[p] - to_call
-			if remaining_after_call > 0:
-				pot_after_call = ps.pot_size + to_call
-				half = max(min_raise_inc, int(pot_after_call * 0.5))
-				if remaining_after_call >= half and half > 0:
-					out.append(ActionType.HALF_POT_BET)
-				pot = max(min_raise_inc, int(pot_after_call))
-				if remaining_after_call >= pot and pot > 0:
-					out.append(ActionType.POT_SIZED_BET)
-				if mode == "sparse_3":
-					two = max(min_raise_inc, int(pot_after_call * 2.0))
-					if remaining_after_call >= two and two > 0:
-						out.append(ActionType.TWO_POT_BET)
-			out.append(ActionType.ALL_IN)
-		return out
-
-
-	def _allowed_actions_opponent(self, ps):
-		out = []
-		p = ps.current_player
-		o = (p + 1) % 2
-		my_bet = ps.current_bets[p]
-		opp_bet = ps.current_bets[o]
-		to_call = opp_bet - my_bet
-		if to_call < 0:
-			to_call = 0
-		min_raise_inc = ps._min_raise_size()
-		if to_call > 0:
-			out.append(ActionType.FOLD)
-		out.append(ActionType.CALL)
-		if to_call == 0:
-			if ps.stacks[p] > 0:
-				pot = max(min_raise_inc, int(ps.pot_size))
-				if ps.stacks[p] >= pot and pot > 0:
-					out.append(ActionType.POT_SIZED_BET)
-				out.append(ActionType.ALL_IN)
-		else:
-			remaining_after_call = ps.stacks[p] - to_call
-			if remaining_after_call > 0:
-				pot_after_call = ps.pot_size + to_call
-				pot = max(min_raise_inc, int(pot_after_call))
-				if remaining_after_call >= pot and pot > 0:
-					out.append(ActionType.POT_SIZED_BET)
-			out.append(ActionType.ALL_IN)
-		return out
-	def _state_key(self, node):
-		if hasattr(node, "_public_signature"):
-			return node._public_signature()
-		ps = node.public_state
-		return (tuple(ps.board_cards), int(ps.current_round), (int(ps.current_bets[0]), int(ps.current_bets[1])), int(ps.pot_size), int(ps.current_player), int(ps.dealer), bool(ps.is_terminal), bool(ps.is_showdown), (bool(ps.players_in_hand[0]), bool(ps.players_in_hand[1])))
-
-
-
-	def _compatible_cluster_mask(self, board_cards):
-		bset = set(board_cards)
-		mask = {}
-		for cid, hset in self.clusters.items():
-			ok = False
-			for h in hset:
-				if isinstance(h, str):
-					c1, c2 = h.split()
-				else:
-					c1, c2 = list(h)
-				if (c1 not in bset) and (c2 not in bset) and (c1 != c2):
-					ok = True
-					break
-			mask[int(cid)] = ok
-		return mask
-
-
-	def _filter_range_by_board(self, rng, board_cards):
-		mask = self._compatible_cluster_mask(board_cards)
-		out = {}
-		total = 0.0
-		for cid, p in rng.items():
-			c = int(cid)
-			v = float(p) if mask.get(c, False) else 0.0
-			out[c] = v
-			total += v
-		if total > 0.0:
-			for c in list(out.keys()):
-				out[c] = out[c] / total
-		return out
-
-	def update_tracking_on_own_action(self, node, agent_player, counterfactual_values):
-		if not hasattr(self, "own_range_tracking"):
-			self.own_range_tracking = {}
-		if not hasattr(self, "opponent_cfv_upper_tracking"):
-			self.opponent_cfv_upper_tracking = {}
-		key = self._state_key(node)
-		self.own_range_tracking[key] = dict(node.player_ranges[agent_player])
-		opp = (agent_player + 1) % 2
-		opp_cfvs = counterfactual_values.get(opp, {})
-		upper = {}
-		for cid, vals in opp_cfvs.items():
-			if isinstance(vals, (list, tuple)) and len(vals) > 0:
-				m = float(max(vals))
-			else:
-				m = float(vals) if isinstance(vals, (int, float)) else 0.0
-			upper[int(cid)] = m
-		if key in self.opponent_cfv_upper_tracking:
-			prev = self.opponent_cfv_upper_tracking[key]
-			for c, v in upper.items():
-				if c in prev:
-					prev[c] = max(prev[c], v)
-				else:
-					prev[c] = v
-			self.opponent_cfv_upper_tracking[key] = prev
-		else:
-			self.opponent_cfv_upper_tracking[key] = upper
-
-
-
-	def update_tracking_on_chance_action(self, node, agent_player):
-		if not hasattr(self, "own_range_tracking"):
-			self.own_range_tracking = {}
-		if not hasattr(self, "opponent_cfv_upper_tracking"):
-			self.opponent_cfv_upper_tracking = {}
-		key = self._state_key(node)
-		prev_keys = list(self.own_range_tracking.keys())
-		if prev_keys:
-			last_key = prev_keys[-1]
-			rng_prev = self.own_range_tracking[last_key]
-			self.own_range_tracking[key] = self._filter_range_by_board(rng_prev, node.public_state.board_cards)
-		else:
-			self.own_range_tracking[key] = dict(node.player_ranges[agent_player])
-		prev_keys2 = list(self.opponent_cfv_upper_tracking.keys())
-		if prev_keys2:
-			last_key2 = prev_keys2[-1]
-			upper_prev = self.opponent_cfv_upper_tracking[last_key2]
-		else:
-			upper_prev = {}
-		mask = self._compatible_cluster_mask(node.public_state.board_cards)
-		upper_new = {}
-		for c, v in upper_prev.items():
-			if mask.get(int(c), False):
-				upper_new[int(c)] = float(v)
-			else:
-				upper_new[int(c)] = 0.0
-		self.opponent_cfv_upper_tracking[key] = upper_new
-
-
-
-	def update_tracking_on_opponent_action(self, node, agent_player):
-		if not hasattr(self, "own_range_tracking"):
-			self.own_range_tracking = {}
-		if not hasattr(self, "opponent_cfv_upper_tracking"):
-			self.opponent_cfv_upper_tracking = {}
-		key = self._state_key(node)
-		prev_keys = list(self.own_range_tracking.keys())
-		if prev_keys:
-			self.own_range_tracking[key] = dict(self.own_range_tracking[prev_keys[-1]])
-		else:
-			self.own_range_tracking[key] = dict(node.player_ranges[agent_player])
-		prev_keys2 = list(self.opponent_cfv_upper_tracking.keys())
-		if prev_keys2:
-			self.opponent_cfv_upper_tracking[key] = dict(self.opponent_cfv_upper_tracking[prev_keys2[-1]])
-		else:
-			self.opponent_cfv_upper_tracking[key] = {}
-
-
-
-	def _mixed_action_distribution(self, node, player, allowed_actions):
-		num_actions = len(allowed_actions)
-		if num_actions == 0:
-			return []
-		values = self.cfr_values[node]
-		priors = node.player_ranges[player]
-		w_sum = 0.0
-		acc = [0.0] * len(ActionType)
-		for cid, p in priors.items():
-			strat = values.get_average_strategy(cid)
-			m = self._mask_strategy(strat, allowed_actions)
-			i = 0
-			while i < len(m):
-				acc[i] += float(p) * float(m[i])
-				i += 1
-			w_sum += float(p)
-		if w_sum <= 0.0:
-			u = 1.0 / float(num_actions)
-			return [u] * num_actions
-		total = 0.0
-		probs = []
-		for a_type in allowed_actions:
-			pi = acc[a_type.value] / w_sum
-			if pi < 0.0:
-				pi = 0.0
-			probs.append(pi)
-			total += pi
-		if total <= 0.0:
-			u = 1.0 / float(num_actions)
-			return [u] * num_actions
-		i = 0
-		while i < len(probs):
-			probs[i] = probs[i] / total
-			i += 1
-		return probs
-
-
-	def _opponent_node_value_from_upper_bounds(self, node, agent_player):
-		if not hasattr(self, "opponent_cfv_upper_tracking"):
-			return 0.0
-		key = self._state_key(node) if hasattr(self, "_state_key") else None
-		if key is None or key not in self.opponent_cfv_upper_tracking:
-			return 0.0
-		upper = self.opponent_cfv_upper_tracking[key]
-		if not upper:
-			return 0.0
-		mx = None
-		for v in upper.values():
-			fv = float(v)
-			if mx is None or fv > mx:
-				mx = fv
-		if mx is None:
-			return 0.0
-		return -float(mx)
+                            post[cid] = post[cid] / norm
+            node.player_ranges[player] = post
+
