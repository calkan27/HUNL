--- a/value_server.py
+++ b/value_server.py
@@ -7,75 +7,17 @@
 
 class _ResultHandle:
     def __init__(self, models: Dict[str, torch.nn.Module], device: Optional[torch.device] = None, max_batch_size: int = 1024, max_wait_ms: int = 2):
-        self.models = {str(k): v for k, v in models.items()}
-        self.device = device if device is not None else torch.device("cuda" if torch.cuda.is_available() else "cpu")
-        for k in list(self.models.keys()):
-            self.models[k] = self.models[k].to(self.device)
-            self.models[k].eval()
-        self.max_batch = int(max_batch_size)
-        self.max_wait_ms = int(max_wait_ms)
-        self._q: "queue.Queue[Tuple[str, torch.Tensor, bool, _ResultHandle]]" = queue.Queue()
-        self._stop = threading.Event()
-        self._thr: Optional[threading.Thread] = None
-        self._counters: Dict[str, int] = {"preflop": 0, "flop": 0, "turn": 0, "river": 0}
-
-	def set(self, value: Tuple[torch.Tensor, torch.Tensor]) -> None:
-		self._out = value
-		self._evt.set()
-	def result(self, as_numpy: bool = True) -> Tuple[Any, Any]:
-		self._evt.wait()
-		v1, v2 = self._out
-		if as_numpy:
-			return v1.detach().cpu().numpy(), v2.detach().cpu().numpy()
-		return v1, v2
-
-class ValueServer:
-	def __init__(self, models: Dict[str, torch.nn.Module], device: Optional[torch.device] = None, max_batch_size: int = 1024, max_wait_ms: int = 2):
-		self.models = {str(k): v for k, v in models.items()}
-		self.device = device if device is not None else torch.device("cuda" if torch.cuda.is_available() else "cpu")
-		for k in list(self.models.keys()):
-			self.models[k] = self.models[k].to(self.device)
-			self.models[k].eval()
-		self.max_batch = int(max_batch_size)
-		self.max_wait_ms = int(max_wait_ms)
-		self._q: "queue.Queue[Tuple[str, torch.Tensor, bool, _ResultHandle]]" = queue.Queue()
-		self._stop = threading.Event()
-		self._thr: Optional[threading.Thread] = None
-
-	def start(self) -> None:
-		if self._thr is not None and self._thr.is_alive():
-			return
-		self._stop.clear()
-		self._thr = threading.Thread(target=self._run, name="ValueServerWorker", daemon=True)
-		self._thr.start()
-
-	def stop(self, join: bool = True) -> None:
-		self._stop.set()
-		if join and self._thr is not None:
-			self._thr.join(timeout=1.0)
-
-	def _run(self) -> None:
-		while not self._stop.is_set():
-			try:
-				req = self._q.get(timeout=0.001)
-			except queue.Empty:
-				continue
-			stage0, x0, scale_to_pot0, h0 = req
-			stage_batch: List[str] = [stage0]
-			xs: List[torch.Tensor] = [x0]
-			scale_flags: List[bool] = [scale_to_pot0]
-			handles: List[_ResultHandle] = [h0]
-			t_deadline = time.time() + (self.max_wait_ms / 1000.0)
-			while len(xs) < self.max_batch and time.time() < t_deadline:
-				try:
-					stg, xt, sc, hh = self._q.get_nowait()
-				except queue.Empty:
-					break
-				stage_batch.append(stg)
-				xs.append(xt)
-				scale_flags.append(sc)
-				handles.append(hh)
-			self._process_batch(stage_batch, xs, scale_flags, handles)
+            self.models = {str(k): v for k, v in models.items()}
+            self.device = device if device is not None else torch.device("cuda" if torch.cuda.is_available() else "cpu")
+            for k in list(self.models.keys()):
+                    self.models[k] = self.models[k].to(self.device)
+                    self.models[k].eval()
+            self.max_batch = int(max_batch_size)
+            self.max_wait_ms = int(max_wait_ms)
+            self._q = queue.Queue()
+            self._stop = threading.Event()
+            self._thr: Optional[threading.Thread] = None
+            self._counters: Dict[str, int] = {"preflop": 0, "flop": 0, "turn": 0, "river": 0}
 
     def _process_batch(self, stages: List[str], xs: List[torch.Tensor], scale_flags: List[bool], handles: List[_ResultHandle]) -> None:
         idx_by_stage: Dict[str, List[int]] = {}
