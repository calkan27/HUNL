--- a/cfr_solver.py
+++ b/cfr_solver.py
@@ -331,69 +331,79 @@
         cache[node_key] = counterfactual_values
         return counterfactual_values
 
-	def _calculate_counterfactual_utility(self, node, player, depth):
-		if self._is_terminal(node):
-			return self._calculate_terminal_utility(node, player)
-		stage = self.get_stage(node)
-		if depth >= self.depth_limit and stage in ('preflop', 'flop'):
-			preds = self.predict_counterfactual_values(node, player)
-			sc = 1.0 if bool(getattr(self, "_label_pot_fraction", False)) else float(node.public_state.pot_size)
-			ev = 0.0
-			total = 0.0
-			for cid, p in node.player_ranges[player].items():
-				if cid in preds:
-					v = preds[cid][0]
-					if isinstance(v, (list, tuple)):
-						val = float(v[0]) * sc
-					else:
-						val = float(v) * sc
-					ev += p * val
-					total += p
-			if total > 0:
-				return ev / total
-			else:
-				return 0.0
-		current_player = node.current_player
-		expected_value = 0.0
-		if current_player == player:
-			allowed_actions = self._allowed_actions_agent(node.public_state)
-			for cluster_id, cluster_prob in node.player_ranges[player].items():
-				if cluster_prob == 0.0:
-					continue
-				values = self.cfr_values[node]
-				base_strategy = values.compute_strategy(cluster_id)
-				strategy = self._mask_strategy(base_strategy, allowed_actions)
-				action_utilities = []
-				for a_type in allowed_actions:
-					a_idx = a_type.value
-					if a_idx in values.pruned_actions[cluster_id]:
-						action_utilities.append(0.0)
-						continue
-					action = Action(a_type)
-					new_public_state = node.public_state.update_state(node, action)
-					child_node = GameNode(new_public_state)
-					child_node.player_ranges = copy.deepcopy(node.player_ranges)
-					self.update_player_range(child_node, player, cluster_id, a_idx)
-					utility = self._calculate_counterfactual_utility(child_node, player, depth + 1)
-					action_utilities.append(utility)
-				i = 0
-				while i < len(allowed_actions):
-					a_idx = allowed_actions[i].value
-					expected_value += cluster_prob * strategy[a_idx] * action_utilities[i]
-					i += 1
-		else:
-			allowed_actions = self._allowed_actions_opponent(node.public_state)
-			best = None
-			for a_type in allowed_actions:
-				act = Action(a_type)
-				ps2 = node.public_state.update_state(node, act)
-				ch = GameNode(ps2)
-				ch.player_ranges = copy.deepcopy(node.player_ranges)
-				val = self._calculate_counterfactual_utility(ch, player, depth + 1)
-				if best is None or val < best:
-					best = val
-			expected_value += float(best if best is not None else 0.0)
-		return expected_value
+    def _calculate_counterfactual_utility(self, node, player, depth):
+        if self._is_terminal(node):
+            return self._calculate_terminal_utility(node, player)
+        stage = self.get_stage(node)
+        if depth >= self.depth_limit and stage in ('preflop', 'flop'):
+            preds = self.predict_counterfactual_values(node, player)
+            sc = 1.0 if bool(getattr(self, "_label_pot_fraction", False)) else float(node.public_state.pot_size)
+            ev = 0.0
+            total = 0.0
+            for cid, p in node.player_ranges[player].items():
+                if cid in preds:
+                    v = preds[cid][0]
+                    if isinstance(v, (list, tuple)):
+                        val = float(v[0]) * sc
+                    else:
+                        val = float(v) * sc
+                    ev += p * val
+                    total += p
+            if total > 0:
+                return ev / total
+            else:
+                return 0.0
+        current_player = node.current_player
+        expected_value = 0.0
+        if current_player == player:
+            allowed_actions = self._allowed_actions_agent(node.public_state)
+            for cluster_id, cluster_prob in node.player_ranges[player].items():
+                if cluster_prob == 0.0:
+                    continue
+                values = self.cfr_values[node]
+                base_strategy = values.compute_strategy(cluster_id)
+                strategy = self._mask_strategy(base_strategy, allowed_actions)
+                action_utilities = []
+                for a_type in allowed_actions:
+                    a_idx = a_type.value
+                    if a_idx in values.pruned_actions[cluster_id]:
+                        action_utilities.append(0.0)
+                        continue
+                    action = Action(a_type)
+                    ps2 = node.public_state.update_state(node, action)
+                    child_node = GameNode(ps2)
+                    child_node.player_ranges = copy.deepcopy(node.player_ranges)
+                    if int(ps2.current_round) > int(node.public_state.current_round):
+                        try:
+                            self.lift_ranges_after_chance(child_node)
+                        except Exception:
+                            pass
+                    self.update_player_range(child_node, player, cluster_id, a_idx)
+                    utility = self._calculate_counterfactual_utility(child_node, player, depth + 1)
+                    action_utilities.append(utility)
+                i = 0
+                while i < len(allowed_actions):
+                    a_idx = allowed_actions[i].value
+                    expected_value += cluster_prob * strategy[a_idx] * action_utilities[i]
+                    i += 1
+        else:
+            allowed_actions = self._allowed_actions_opponent(node.public_state)
+            best = None
+            for a_type in allowed_actions:
+                act = Action(a_type)
+                ps2 = node.public_state.update_state(node, act)
+                ch = GameNode(ps2)
+                ch.player_ranges = copy.deepcopy(node.player_ranges)
+                if int(ps2.current_round) > int(node.public_state.current_round):
+                    try:
+                        self.lift_ranges_after_chance(ch)
+                    except Exception:
+                        pass
+                val = self._calculate_counterfactual_utility(ch, player, depth + 1)
+                if best is None or val < best:
+                    best = val
+            expected_value += float(best if best is not None else 0.0)
+        return expected_value
 
 	def _update_regret(self, node, player, cfv_by_action):
 		values = self.cfr_values[node]
