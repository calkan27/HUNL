--- a/agent.py
+++ b/agent.py
@@ -60,17 +60,36 @@
 	def _public_key(self, ps):
 		return (tuple(ps.board_cards), int(ps.current_round), (int(ps.current_bets[0]), int(ps.current_bets[1])), int(ps.pot_size), int(ps.current_player) if ps.current_player is not None else -1, int(ps.dealer), bool(ps.is_terminal), bool(ps.is_showdown), (bool(ps.players_in_hand[0]), bool(ps.players_in_hand[1])))
 
-	def act(self, public_state, our_private_cards):
-		node = GameNode(public_state)
-		cid = self._bucketize_own_hand(list(our_private_cards), list(public_state.board_cards))
-		r_self = self._range_on_bucket(cid)
-		r_opp = self._uniform_range()
-		node.player_ranges[public_state.current_player] = r_self
-		node.player_ranges[(public_state.current_player + 1) % 2] = r_opp
-		self.solver.total_iterations = int(getattr(self.solver, "_round_iters", {}).get(int(public_state.current_round), getattr(self._config, "total_iterations", 1000)))
-		act = self.solver.run_cfr(node)
-		self.last_public_key = self._public_key(node.public_state)
-		return act
+    def act(self, public_state, our_private_cards):
+        node = GameNode(public_state)
+        cid = self._bucketize_own_hand(list(our_private_cards), list(public_state.board_cards))
+
+        def _beliefs_for_node(nd, own_cid):
+            key = self._public_key(nd.public_state)
+            r_self = None
+            if hasattr(self.solver, "own_range_tracking") and key in getattr(self.solver, "own_range_tracking", {}):
+                try:
+                    track = dict(self.solver.own_range_tracking[key])
+                    s = sum(float(v) for v in track.values())
+                    if s > 0.0:
+                        for k in list(track.keys()):
+                            track[k] = float(track[k]) / s
+                    r_self = {int(k): float(v) for k, v in track.items()}
+                except Exception:
+                    r_self = None
+            if r_self is None:
+                r_self = self._range_on_bucket(own_cid)
+            r_opp = self._uniform_range()
+            return r_self, r_opp
+
+        r_self, r_opp = _beliefs_for_node(node, cid)
+        node.player_ranges[public_state.current_player] = r_self
+        node.player_ranges[(public_state.current_player + 1) % 2] = r_opp
+
+        self.solver.total_iterations = int(getattr(self.solver, "_round_iters", {}).get(int(public_state.current_round), getattr(self._config, "total_iterations", 1000)))
+        act = self.solver.run_cfr(node)
+        self.last_public_key = self._public_key(node.public_state)
+        return act
 
 	def observe_opponent_action(self, prev_public_state, new_public_state, observed_action_type):
 		prev_node = GameNode(prev_public_state)
